{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import required packages\n",
    "The following code is based on https://github.com/shravankumar147/gtsrb-smai/blob/master/docs/MLP%2BClassfier%2Bon%2BHOG%2BFeatures/MLP%20Classfier%20on%20HOG%20Features.md with modificaitons for Python3 support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "from PIL import Image\n",
    "from skimage import exposure, feature, transform\n",
    "%matplotlib inline\n",
    "\n",
    "# classification required packages\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "# function for reading the images\n",
    "# arguments: path to the traffic sign data, for example '../../GTSRB/train/Final_Training/Images/'\n",
    "# returns: list of images, list of corresponding labels \n",
    "def readTrafficSigns(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example '../dataset/GTSRB/train/Final_Training/Images/'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "def get_csv(path):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "def showimg_n_hog(grayimg,hogImage):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(grayimg)\n",
    "    ax1.set_title('Input image')\n",
    "    ax1.set_adjustable('box')\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hogImage, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    ax1.set_adjustable('box')\n",
    "    plt.show()\n",
    "    \n",
    "# Functions for testimages    \n",
    "def loadtestimages_from_path(testpath):\n",
    "    print(\"[INFO] reading all test images from directory\")\n",
    "    gtFile = get_csv(testpath)\n",
    "    filename = gtFile[0]\n",
    "    raw_data = open(filename, 'rt')\n",
    "    reader = csv.reader(raw_data, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    next(reader)\n",
    "    testfiles = list(reader)\n",
    "    timg = []\n",
    "    testimg = []\n",
    "    tlbl = []\n",
    "    for i in testfiles:\n",
    "    #     print (i[0],i[-1])\n",
    "        fname = os.path.join(testpath,i[0])\n",
    "        timg.append(fname)\n",
    "        tim = plt.imread(fname)\n",
    "        testimg.append(tim)\n",
    "        label = i[-1]\n",
    "        tlbl.append(label)\n",
    "    np.save(\"Image_n_Labels/testimagenames.npy\",timg)\n",
    "    np.save(\"Image_n_Labels/testimages.npy\",testimg)\n",
    "    np.save(\"Image_n_Labels/testimagelabels.npy\",tlbl)\n",
    "    return timg, testimg, tlbl\n",
    "    \n",
    "def loadtestimages_from_npy():\n",
    "    print(\"[INFO] loading testing images\")\n",
    "    timg = np.load(\"Image_n_Labels/testimagenames.npy\", allow_pickle=True)\n",
    "    testimg = np.load(\"Image_n_Labels/testimages.npy\", allow_pickle=True)\n",
    "    tlbl = np.load(\"Image_n_Labels/testimagelabels.npy\")\n",
    "    return timg, testimg, tlbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training images and labels are loaded in variables ==> X,y\n",
      "[INFO] Number of training Images 39209 \n",
      "Number of Labels 39209\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Image_n_Labels/trainImages.npy\") &  os.path.isfile(\"Image_n_Labels/trainLabels.npy\") :\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\", allow_pickle=True)\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\", allow_pickle=True)\n",
    "    print(\"[INFO] Training images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"[INFO] Number of training Images {} \\nNumber of Labels {}\".format(len(X), len(y)))\n",
    "else:    \n",
    "    # training images and labels\n",
    "    trainImages, trainLabels = readTrafficSigns('../GTSRB/Final_Training/Images/')\n",
    "    np.save(\"Image_n_Labels/trainImages.npy\",trainImages)\n",
    "    np.save(\"Image_n_Labels/trainLabels.npy\",trainLabels)\n",
    "    print(\"[INFO] training images and labels are read from the dataset directory\")\n",
    "    print(\"[INFO] training images saved to Image_n_Labels/trainingImages.npy for further use\")\n",
    "    print(\"[INFO] training labels saved to Image_n_Labels/trainingLabels.npy for further use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568\n"
     ]
    }
   ],
   "source": [
    "# simple test on single image for HoG features\n",
    "n = np.random.randint(0,len(X))\n",
    "i1 = X[n]\n",
    "grayim = rgb2gray(i1)\n",
    "gI1 = transform.resize(grayim,(40,40))\n",
    "# gI2 = cv2.resize(grayim, (40, 40), interpolation = cv2.INTER_CUBIC)\n",
    "# Original parms\n",
    "# (H, hogImage) = feature.hog(gI1, orientations=9, pixels_per_cell=(8,8),\n",
    "#     cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "\n",
    "# HOG 2 from http://www.lara.prd.fr/_media/users/ijcnn.pdf\n",
    "(H, hogImage) = feature.hog(gI1, orientations=8, pixels_per_cell=(5,5),\n",
    "                            cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "\n",
    "showimg_n_hog(gI1, hogImage)\n",
    "print(len(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading testing images\n"
     ]
    }
   ],
   "source": [
    "testpath=\"../GTSRB/Final_Test/Images/\"\n",
    "if os.path.isfile(\"Image_n_Labels/testimagenames.npy\") and os.path.isfile(\"Image_n_Labels/testimages.npy\") and os.path.isfile(\"Image_n_Labels/testimagelabels.npy\"):\n",
    "    timg, testimg, tlbl = loadtestimages_from_npy()\n",
    "else:\n",
    "    timg, testimg, tlbl = loadtestimages_from_path(testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save HOG For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoG2features.npy does not found\n",
      "[INFO] processed 1000/39209\n",
      "[INFO] processed 2000/39209\n",
      "[INFO] processed 3000/39209\n",
      "[INFO] processed 4000/39209\n",
      "[INFO] processed 5000/39209\n",
      "[INFO] processed 6000/39209\n",
      "[INFO] processed 7000/39209\n",
      "[INFO] processed 8000/39209\n",
      "[INFO] processed 9000/39209\n",
      "[INFO] processed 10000/39209\n",
      "[INFO] processed 11000/39209\n",
      "[INFO] processed 12000/39209\n",
      "[INFO] processed 13000/39209\n",
      "[INFO] processed 14000/39209\n",
      "[INFO] processed 15000/39209\n",
      "[INFO] processed 16000/39209\n",
      "[INFO] processed 17000/39209\n",
      "[INFO] processed 18000/39209\n",
      "[INFO] processed 19000/39209\n",
      "[INFO] processed 20000/39209\n",
      "[INFO] processed 21000/39209\n",
      "[INFO] processed 22000/39209\n",
      "[INFO] processed 23000/39209\n",
      "[INFO] processed 24000/39209\n",
      "[INFO] processed 25000/39209\n",
      "[INFO] processed 26000/39209\n",
      "[INFO] processed 27000/39209\n",
      "[INFO] processed 28000/39209\n",
      "[INFO] processed 29000/39209\n",
      "[INFO] processed 30000/39209\n",
      "[INFO] processed 31000/39209\n",
      "[INFO] processed 32000/39209\n",
      "[INFO] processed 33000/39209\n",
      "[INFO] processed 34000/39209\n",
      "[INFO] processed 35000/39209\n",
      "[INFO] processed 36000/39209\n",
      "[INFO] processed 37000/39209\n",
      "[INFO] processed 38000/39209\n",
      "[INFO] processed 39000/39209\n",
      "HoG2features.npy are saved\n",
      "HoG2visualize.npy are saved\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"HoGFeatures/HoG2features.npy\") & os.path.isfile(\"HoGFeatures/HoG2visualize.npy\") :\n",
    "    print(\"loading from file ... \")\n",
    "    hogfeat = np.load(\"HoGFeatures/HoG2features.npy\")\n",
    "    hogviz = np.load(\"HoGFeatures/HoG2visualize.npy\")\n",
    "    \n",
    "    print(\"HoG features are loaded from HoG2features.npy to variable ==> hogfeat\")\n",
    "    print(\"HoG visualizations are loaded from HoGvisualize.npy to variable ==> hogviz\")\n",
    "else:\n",
    "    print(\"HoG2features.npy does not found\")\n",
    "    Hviz = []\n",
    "    Hfeat = []\n",
    "    for i in range(0,len(X)):\n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(X)))\n",
    "        I = X[i]\n",
    "        grayim = rgb2gray(I)\n",
    "        grayim = transform.resize(grayim,(40,40))\n",
    "\n",
    "        # TODO: try HOG2 from http://www.lara.prd.fr/_media/users/ijcnn.pdf\n",
    "        (H_5x5, hogImage) = feature.hog(grayim, orientations=8, pixels_per_cell=(5, 5),\n",
    "            cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "#         hogImage = hogImage.astype(\"uint8\")\n",
    "        Hviz.append(hogImage)\n",
    "        Hfeat.append(H_5x5)\n",
    "        # save the features using numpy save with .npy extention \n",
    "        # which reduced the storage space by 4times compared to pickle\n",
    "    np.save(\"HoGFeatures/HoG2features.npy\", Hfeat)\n",
    "    np.save(\"HoGFeatures/HoG2visualize.npy\", Hviz)\n",
    "    print(\"HoG2features.npy are saved\")  \n",
    "    print(\"HoG2visualize.npy are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save HOG For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoGfeatures_test.npy does not found\n",
      "[INFO] processed 1000/12630\n",
      "[INFO] processed 2000/12630\n",
      "[INFO] processed 3000/12630\n",
      "[INFO] processed 4000/12630\n",
      "[INFO] processed 5000/12630\n",
      "[INFO] processed 6000/12630\n",
      "[INFO] processed 7000/12630\n",
      "[INFO] processed 8000/12630\n",
      "[INFO] processed 9000/12630\n",
      "[INFO] processed 10000/12630\n",
      "[INFO] processed 11000/12630\n",
      "[INFO] processed 12000/12630\n",
      "HoG2features_test.npy are saved\n",
      "HoG2visualize_test.npy are saved\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"HoGFeatures/HoG2features_test.npy\") and os.path.isfile(\"HoGFeatures/HoG2visualize_test.npy\") :\n",
    "    hogfeat_test = np.load(\"HoGFeatures/HoG2features_test.npy\")\n",
    "    hogviz_test = np.load(\"HoGFeatures/HoG2visualize_test.npy\")\n",
    "    \n",
    "    print(\"HoG features are loaded from HoGfeatures_test.npy to variable ==> hogfeat_test\")\n",
    "    print(\"HoG visualizations are loaded from HoGvisualize_test.npy to variable ==> hogviz_test\")\n",
    "else:\n",
    "    print(\"HoG2features_test.npy does not found\")\n",
    "    Hviz = []\n",
    "    Hfeat = []\n",
    "    for i in range(0,len(testimg)):\n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(testimg)))\n",
    "        I = testimg[i]\n",
    "        grayim = rgb2gray(I)\n",
    "        grayim = transform.resize(grayim,(40,40))\n",
    "\n",
    "        (H_5x5, hogImage) = feature.hog(grayim, orientations=8, pixels_per_cell=(5, 5),\n",
    "            cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "        hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "    #         hogImage = hogImage.astype(\"uint8\")\n",
    "        Hviz.append(hogImage)\n",
    "        Hfeat.append(H_5x5)\n",
    "        # save the features using numpy save with .npy extention \n",
    "        # which reduced the storage space by 4times compared to pickle\n",
    "    np.save(\"HoGFeatures/HoG2features_test.npy\", Hfeat)\n",
    "    np.save(\"HoGFeatures/HoG2visualize_test.npy\", Hviz)\n",
    "    print(\"HoG2features_test.npy are saved\")  \n",
    "    print(\"HoG2visualize_test.npy are saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert datatype to float for both training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "Xhog = np.array(hogfeat).astype(\"float\")\n",
    "y = y.astype(\"float\")\n",
    "\n",
    "# Testing\n",
    "X_t = np.array(hogfeat_test).astype(\"float\")\n",
    "tlbl = np.array(tlbl).astype(\"float\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = Xhog\n",
    "labels = y\n",
    "testData = X_t\n",
    "testLabels = tlbl\n",
    "# (trainData, valData, trainLabels, valLabels) = train_test_split(features, labels,\n",
    "#     test_size=0.1, random_state=84)\n",
    "trainData = features\n",
    "trainLabels = y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    with open('knn_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('svm_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_svm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open('svm_linear_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_svm_linear, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('knn_model.pickle', 'rb') as handle:\n",
    "        clf = pickle.load(handle)\n",
    "    \n",
    "    with open('svm_model.pickle', 'rb') as handle:\n",
    "        clf_svm = pickle.load(handle)\n",
    "        \n",
    "    with open('svm_linear_model.pickle', 'rb') as handle:\n",
    "        clf_svm_linear = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-0d4b75f9c289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclf_svm_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclf_svm_linear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             self.loss, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"crammer_singer\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misspmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         epsilon, sample_weight)\n\u001b[0m\u001b[1;32m    922\u001b[0m     \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf_svm_linear = svm.LinearSVC(max_iter=10000)\n",
    "clf_svm_linear.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear2 = svm.LinearSVC(max_iter=100000)\n",
    "clf_svm_linear2.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear3 = svm.LinearSVC(max_iter=10000, loss='hinge')\n",
    "clf_svm_linear3.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_3.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear4 = svm.LinearSVC(max_iter=10000, tol=1e-5)\n",
    "clf_svm_linear4.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_4.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear5 = svm.LinearSVC(max_iter=100000, tol=1e-5)\n",
    "clf_svm_linear5.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_5.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear5, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_rfc = ensemble.RandomForestClassifier()\n",
    "clf_rfc.fit(trainData, trainLabels)\n",
    "with open('rfc_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc2 = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "clf_rfc2.fit(trainData, trainLabels)\n",
    "with open('rfc_model2.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc3 = ensemble.RandomForestClassifier(n_estimators=200)\n",
    "clf_rfc3.fit(trainData, trainLabels)\n",
    "with open('rfc_model3.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc4 = ensemble.RandomForestClassifier(n_estimators=500)\n",
    "clf_rfc4.fit(trainData, trainLabels)\n",
    "with open('rfc_model4.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc5 = ensemble.RandomForestClassifier(min_samples_split=500, n_estimators=500, n_jobs=8)\n",
    "clf_rfc5.fit(trainData, trainLabels)\n",
    "with open('rfc_model5.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc5, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc6 = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=8)\n",
    "clf_rfc6.fit(trainData, trainLabels)\n",
    "with open('rfc_model6.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc6, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With HOG2\n",
    "clf_rfc7 = ensemble.RandomForestClassifier(n_estimators=500, n_jobs=8)\n",
    "clf_rfc7.fit(trainData, trainLabels)\n",
    "with open('rfc_model7.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc7, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = clf.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7969912905779889\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.38      0.50        60\n",
      "         1.0       0.78      0.64      0.70       720\n",
      "         2.0       0.70      0.70      0.70       750\n",
      "         3.0       0.59      0.66      0.62       450\n",
      "         4.0       0.77      0.90      0.83       660\n",
      "         5.0       0.48      0.67      0.56       630\n",
      "         6.0       0.89      0.73      0.80       150\n",
      "         7.0       0.73      0.69      0.71       450\n",
      "         8.0       0.66      0.70      0.68       450\n",
      "         9.0       0.92      0.78      0.84       480\n",
      "        10.0       0.84      0.93      0.88       660\n",
      "        11.0       0.79      0.75      0.77       420\n",
      "        12.0       1.00      1.00      1.00       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.91      0.95       270\n",
      "        15.0       0.99      0.95      0.97       210\n",
      "        16.0       0.91      0.98      0.94       150\n",
      "        17.0       1.00      0.94      0.97       360\n",
      "        18.0       0.82      0.74      0.78       390\n",
      "        19.0       0.68      0.83      0.75        60\n",
      "        20.0       0.88      0.76      0.81        90\n",
      "        21.0       0.66      0.64      0.65        90\n",
      "        22.0       0.59      0.69      0.64       120\n",
      "        23.0       0.62      0.54      0.58       150\n",
      "        24.0       0.62      0.66      0.64        90\n",
      "        25.0       0.75      0.72      0.74       480\n",
      "        26.0       0.51      0.68      0.58       180\n",
      "        27.0       0.67      0.80      0.73        60\n",
      "        28.0       0.52      0.41      0.46       150\n",
      "        29.0       0.60      0.50      0.55        90\n",
      "        30.0       0.56      0.25      0.35       150\n",
      "        31.0       0.65      0.97      0.78       270\n",
      "        32.0       0.97      0.95      0.96        60\n",
      "        33.0       0.99      0.94      0.96       210\n",
      "        34.0       0.96      0.84      0.90       120\n",
      "        35.0       1.00      0.93      0.96       390\n",
      "        36.0       0.98      0.95      0.97       120\n",
      "        37.0       1.00      0.60      0.75        60\n",
      "        38.0       1.00      0.91      0.95       690\n",
      "        39.0       1.00      0.97      0.98        90\n",
      "        40.0       0.99      0.80      0.88        90\n",
      "        41.0       0.86      0.60      0.71        60\n",
      "        42.0       0.85      0.76      0.80        90\n",
      "\n",
      "    accuracy                           0.80     12630\n",
      "   macro avg       0.80      0.76      0.77     12630\n",
      "weighted avg       0.81      0.80      0.80     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_svm = clf_svm.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7781472684085511\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.72      0.73      0.72       720\n",
      "         2.0       0.51      0.96      0.67       750\n",
      "         3.0       0.98      0.36      0.53       450\n",
      "         4.0       0.92      0.94      0.93       660\n",
      "         5.0       0.66      0.63      0.64       630\n",
      "         6.0       0.66      0.65      0.65       150\n",
      "         7.0       0.93      0.80      0.86       450\n",
      "         8.0       0.86      0.86      0.86       450\n",
      "         9.0       0.91      0.89      0.90       480\n",
      "        10.0       0.87      0.95      0.91       660\n",
      "        11.0       0.52      0.94      0.67       420\n",
      "        12.0       0.98      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.87      0.93       270\n",
      "        15.0       0.99      0.91      0.95       210\n",
      "        16.0       1.00      0.67      0.80       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.66      0.83      0.74       390\n",
      "        19.0       0.00      0.00      0.00        60\n",
      "        20.0       0.00      0.00      0.00        90\n",
      "        21.0       0.00      0.00      0.00        90\n",
      "        22.0       0.00      0.00      0.00       120\n",
      "        23.0       1.00      0.26      0.41       150\n",
      "        24.0       0.00      0.00      0.00        90\n",
      "        25.0       0.46      1.00      0.63       480\n",
      "        26.0       0.00      0.00      0.00       180\n",
      "        27.0       0.00      0.00      0.00        60\n",
      "        28.0       1.00      0.01      0.01       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       1.00      0.03      0.06       150\n",
      "        31.0       0.60      0.97      0.74       270\n",
      "        32.0       1.00      0.50      0.67        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       1.00      0.93      0.97       120\n",
      "        35.0       0.93      0.97      0.95       390\n",
      "        36.0       1.00      0.94      0.97       120\n",
      "        37.0       1.00      0.27      0.42        60\n",
      "        38.0       0.96      1.00      0.98       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       1.00      0.56      0.71        90\n",
      "        41.0       1.00      0.23      0.38        60\n",
      "        42.0       1.00      0.01      0.02        90\n",
      "\n",
      "    accuracy                           0.78     12630\n",
      "   macro avg       0.70      0.57      0.57     12630\n",
      "weighted avg       0.78      0.78      0.74     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_svm_linear = clf_svm_linear.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear2 = clf_svm_linear2.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear2)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9208234362628662\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.90      0.86      0.88       720\n",
      "         2.0       0.85      0.93      0.89       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.82      0.81      0.82       630\n",
      "         6.0       0.83      0.77      0.80       150\n",
      "         7.0       0.94      0.91      0.92       450\n",
      "         8.0       0.84      0.85      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.90      0.92       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.90      1.00      0.95       210\n",
      "        16.0       0.95      0.99      0.97       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.90      0.94       390\n",
      "        19.0       0.92      1.00      0.96        60\n",
      "        20.0       0.90      0.89      0.89        90\n",
      "        21.0       0.93      0.78      0.85        90\n",
      "        22.0       1.00      0.79      0.88       120\n",
      "        23.0       0.82      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.90      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.93      0.93      0.93        60\n",
      "        28.0       0.84      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.80      0.55      0.65       150\n",
      "        31.0       0.89      0.98      0.93       270\n",
      "        32.0       0.84      0.78      0.81        60\n",
      "        33.0       0.95      1.00      0.97       210\n",
      "        34.0       0.93      0.99      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.95      1.00      0.98       120\n",
      "        37.0       1.00      0.90      0.95        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.87      0.98      0.92        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.84      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear3 = clf_svm_linear3.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear3)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear4 = clf_svm_linear4.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear4)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear5 = clf_svm_linear5.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear5)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8401425178147268\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.58      0.62        60\n",
      "         1.0       0.71      0.82      0.76       720\n",
      "         2.0       0.68      0.89      0.77       750\n",
      "         3.0       0.79      0.73      0.76       450\n",
      "         4.0       0.86      0.91      0.89       660\n",
      "         5.0       0.65      0.67      0.66       630\n",
      "         6.0       0.85      0.61      0.71       150\n",
      "         7.0       0.83      0.84      0.84       450\n",
      "         8.0       0.77      0.72      0.74       450\n",
      "         9.0       0.87      0.95      0.91       480\n",
      "        10.0       0.93      0.93      0.93       660\n",
      "        11.0       0.76      0.89      0.82       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.98      0.99      0.99       720\n",
      "        14.0       0.95      0.87      0.91       270\n",
      "        15.0       0.95      0.91      0.93       210\n",
      "        16.0       0.99      0.89      0.94       150\n",
      "        17.0       0.99      0.95      0.97       360\n",
      "        18.0       0.84      0.84      0.84       390\n",
      "        19.0       0.77      0.92      0.84        60\n",
      "        20.0       0.78      0.80      0.79        90\n",
      "        21.0       0.69      0.49      0.57        90\n",
      "        22.0       0.92      0.73      0.81       120\n",
      "        23.0       0.72      0.85      0.78       150\n",
      "        24.0       0.84      0.69      0.76        90\n",
      "        25.0       0.81      0.87      0.84       480\n",
      "        26.0       0.76      0.70      0.73       180\n",
      "        27.0       0.76      0.37      0.49        60\n",
      "        28.0       0.72      0.69      0.71       150\n",
      "        29.0       0.61      0.38      0.47        90\n",
      "        30.0       0.51      0.25      0.33       150\n",
      "        31.0       0.90      0.89      0.89       270\n",
      "        32.0       0.93      0.68      0.79        60\n",
      "        33.0       0.93      0.91      0.92       210\n",
      "        34.0       0.92      0.97      0.94       120\n",
      "        35.0       0.96      0.91      0.94       390\n",
      "        36.0       0.92      0.84      0.88       120\n",
      "        37.0       0.98      0.68      0.80        60\n",
      "        38.0       0.95      0.93      0.94       690\n",
      "        39.0       0.96      0.87      0.91        90\n",
      "        40.0       0.92      0.50      0.65        90\n",
      "        41.0       0.82      0.47      0.60        60\n",
      "        42.0       0.96      0.52      0.68        90\n",
      "\n",
      "    accuracy                           0.84     12630\n",
      "   macro avg       0.84      0.76      0.79     12630\n",
      "weighted avg       0.84      0.84      0.84     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc = clf_rfc.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9219319081551861\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.91      0.89      0.90       720\n",
      "         2.0       0.82      0.96      0.89       750\n",
      "         3.0       0.94      0.80      0.86       450\n",
      "         4.0       0.91      0.95      0.93       660\n",
      "         5.0       0.79      0.87      0.83       630\n",
      "         6.0       0.88      0.71      0.79       150\n",
      "         7.0       0.92      0.92      0.92       450\n",
      "         8.0       0.88      0.87      0.88       450\n",
      "         9.0       0.94      0.99      0.96       480\n",
      "        10.0       0.97      0.98      0.98       660\n",
      "        11.0       0.86      0.94      0.90       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.94      0.97       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.93      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.90      0.93       390\n",
      "        19.0       0.95      0.98      0.97        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       0.98      0.72      0.83        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.82      0.94      0.88       150\n",
      "        24.0       0.99      0.87      0.92        90\n",
      "        25.0       0.82      0.98      0.89       480\n",
      "        26.0       0.85      0.76      0.80       180\n",
      "        27.0       1.00      0.58      0.74        60\n",
      "        28.0       0.85      0.95      0.90       150\n",
      "        29.0       0.99      0.83      0.90        90\n",
      "        30.0       0.86      0.45      0.59       150\n",
      "        31.0       0.92      0.98      0.95       270\n",
      "        32.0       0.98      0.95      0.97        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.97      0.97       390\n",
      "        36.0       1.00      0.96      0.98       120\n",
      "        37.0       1.00      0.70      0.82        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.97      0.82      0.89        90\n",
      "        41.0       0.97      0.58      0.73        60\n",
      "        42.0       0.95      0.63      0.76        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.87      0.90     12630\n",
      "weighted avg       0.93      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc2 = clf_rfc2.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc2)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9287410926365796\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.90      0.91      0.91       720\n",
      "         2.0       0.84      0.97      0.90       750\n",
      "         3.0       0.96      0.79      0.87       450\n",
      "         4.0       0.91      0.95      0.93       660\n",
      "         5.0       0.82      0.90      0.86       630\n",
      "         6.0       0.88      0.71      0.79       150\n",
      "         7.0       0.93      0.93      0.93       450\n",
      "         8.0       0.89      0.88      0.88       450\n",
      "         9.0       0.94      0.99      0.96       480\n",
      "        10.0       0.96      0.99      0.97       660\n",
      "        11.0       0.85      0.94      0.89       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.97      0.98       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.94      0.97       150\n",
      "        17.0       1.00      0.99      0.99       360\n",
      "        18.0       0.96      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       1.00      0.83      0.91        90\n",
      "        21.0       0.97      0.71      0.82        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.91      0.96      0.93       150\n",
      "        24.0       1.00      0.88      0.93        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.90      0.78      0.83       180\n",
      "        27.0       0.98      0.68      0.80        60\n",
      "        28.0       0.86      0.97      0.91       150\n",
      "        29.0       1.00      0.87      0.93        90\n",
      "        30.0       0.90      0.53      0.66       150\n",
      "        31.0       0.92      0.98      0.95       270\n",
      "        32.0       0.98      0.97      0.97        60\n",
      "        33.0       0.99      1.00      1.00       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.97      0.97       390\n",
      "        36.0       1.00      0.95      0.97       120\n",
      "        37.0       1.00      0.70      0.82        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.81      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.95      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.88      0.91     12630\n",
      "weighted avg       0.93      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc3 = clf_rfc3.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc3)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9323832145684877\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.57      0.72        60\n",
      "         1.0       0.90      0.90      0.90       720\n",
      "         2.0       0.86      0.97      0.91       750\n",
      "         3.0       0.98      0.81      0.89       450\n",
      "         4.0       0.92      0.96      0.94       660\n",
      "         5.0       0.83      0.92      0.87       630\n",
      "         6.0       0.84      0.69      0.76       150\n",
      "         7.0       0.93      0.94      0.94       450\n",
      "         8.0       0.90      0.89      0.89       450\n",
      "         9.0       0.95      0.99      0.97       480\n",
      "        10.0       0.96      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.98      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.98       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.97      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       0.98      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.87      0.95      0.91       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.87      0.78      0.82       180\n",
      "        27.0       1.00      0.80      0.89        60\n",
      "        28.0       0.91      0.95      0.93       150\n",
      "        29.0       1.00      0.93      0.97        90\n",
      "        30.0       0.90      0.49      0.63       150\n",
      "        31.0       0.90      0.98      0.94       270\n",
      "        32.0       1.00      0.98      0.99        60\n",
      "        33.0       0.97      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.95      0.97       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.96      0.61      0.75        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc4 = clf_rfc4.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc4)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8468725257323833\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.80      0.81      0.81       720\n",
      "         2.0       0.76      0.95      0.84       750\n",
      "         3.0       0.88      0.79      0.84       450\n",
      "         4.0       0.82      0.92      0.87       660\n",
      "         5.0       0.75      0.77      0.76       630\n",
      "         6.0       0.92      0.47      0.62       150\n",
      "         7.0       0.81      0.93      0.86       450\n",
      "         8.0       0.81      0.78      0.79       450\n",
      "         9.0       0.83      0.97      0.89       480\n",
      "        10.0       0.89      0.98      0.93       660\n",
      "        11.0       0.70      0.93      0.80       420\n",
      "        12.0       0.96      1.00      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.99      0.89      0.93       270\n",
      "        15.0       0.96      0.99      0.97       210\n",
      "        16.0       1.00      0.79      0.88       150\n",
      "        17.0       1.00      0.96      0.98       360\n",
      "        18.0       0.81      0.90      0.85       390\n",
      "        19.0       1.00      0.15      0.26        60\n",
      "        20.0       0.99      0.77      0.86        90\n",
      "        21.0       1.00      0.19      0.32        90\n",
      "        22.0       0.97      0.72      0.83       120\n",
      "        23.0       0.70      0.77      0.73       150\n",
      "        24.0       1.00      0.01      0.02        90\n",
      "        25.0       0.63      0.97      0.77       480\n",
      "        26.0       0.83      0.68      0.75       180\n",
      "        27.0       1.00      0.07      0.12        60\n",
      "        28.0       0.93      0.63      0.75       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.79      0.22      0.34       150\n",
      "        31.0       0.73      0.97      0.83       270\n",
      "        32.0       1.00      0.50      0.67        60\n",
      "        33.0       0.97      0.97      0.97       210\n",
      "        34.0       0.98      0.93      0.96       120\n",
      "        35.0       0.92      0.98      0.95       390\n",
      "        36.0       1.00      0.91      0.95       120\n",
      "        37.0       1.00      0.38      0.55        60\n",
      "        38.0       0.93      0.98      0.96       690\n",
      "        39.0       1.00      0.91      0.95        90\n",
      "        40.0       1.00      0.16      0.27        90\n",
      "        41.0       0.00      0.00      0.00        60\n",
      "        42.0       0.97      0.36      0.52        90\n",
      "\n",
      "    accuracy                           0.85     12630\n",
      "   macro avg       0.84      0.68      0.70     12630\n",
      "weighted avg       0.85      0.85      0.83     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc5 = clf_rfc5.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc5)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9338083927157561\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        60\n",
      "         1.0       0.90      0.91      0.91       720\n",
      "         2.0       0.86      0.98      0.91       750\n",
      "         3.0       0.97      0.80      0.87       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.85      0.93      0.88       630\n",
      "         6.0       0.85      0.70      0.77       150\n",
      "         7.0       0.93      0.92      0.93       450\n",
      "         8.0       0.88      0.90      0.89       450\n",
      "         9.0       0.94      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.94      0.97       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.99      0.86      0.92        90\n",
      "        21.0       0.98      0.70      0.82        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.89      0.79      0.84       180\n",
      "        27.0       1.00      0.73      0.85        60\n",
      "        28.0       0.91      0.97      0.94       150\n",
      "        29.0       1.00      0.90      0.95        90\n",
      "        30.0       0.87      0.49      0.63       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.98      0.98      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.99      0.99       120\n",
      "        35.0       0.97      0.98      0.97       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.96      0.61      0.75        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc6 = clf_rfc6.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc6)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9347585114806017\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        60\n",
      "         1.0       0.91      0.91      0.91       720\n",
      "         2.0       0.87      0.97      0.92       750\n",
      "         3.0       0.97      0.81      0.88       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.84      0.93      0.88       630\n",
      "         6.0       0.86      0.73      0.79       150\n",
      "         7.0       0.94      0.94      0.94       450\n",
      "         8.0       0.88      0.89      0.88       450\n",
      "         9.0       0.94      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.89      0.94      0.92       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.96      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.99      0.84      0.91        90\n",
      "        21.0       0.97      0.70      0.81        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.91      0.95        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.88      0.78      0.82       180\n",
      "        27.0       1.00      0.77      0.87        60\n",
      "        28.0       0.88      0.97      0.93       150\n",
      "        29.0       0.99      0.88      0.93        90\n",
      "        30.0       0.87      0.51      0.64       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.97      1.00      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.99      0.98      0.99       120\n",
      "        35.0       0.97      0.98      0.97       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.82      0.90        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       0.97      0.62      0.76        60\n",
      "        42.0       0.97      0.62      0.76        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc7 = clf_rfc7.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc7)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9335708630245447\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.91      0.91      0.91       720\n",
      "         2.0       0.85      0.98      0.91       750\n",
      "         3.0       0.97      0.81      0.88       450\n",
      "         4.0       0.93      0.96      0.94       660\n",
      "         5.0       0.85      0.91      0.88       630\n",
      "         6.0       0.83      0.71      0.77       150\n",
      "         7.0       0.92      0.94      0.93       450\n",
      "         8.0       0.88      0.88      0.88       450\n",
      "         9.0       0.95      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.94      0.97       270\n",
      "        15.0       0.98      0.99      0.99       210\n",
      "        16.0       1.00      0.96      0.98       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.97      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.86      0.91        90\n",
      "        21.0       0.98      0.69      0.81        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.83      0.97      0.90       150\n",
      "        24.0       1.00      0.91      0.95        90\n",
      "        25.0       0.85      0.98      0.91       480\n",
      "        26.0       0.89      0.79      0.84       180\n",
      "        27.0       1.00      0.63      0.78        60\n",
      "        28.0       0.91      0.96      0.93       150\n",
      "        29.0       1.00      0.94      0.97        90\n",
      "        30.0       0.90      0.49      0.64       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      1.00      0.99       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.97      0.98       120\n",
      "        37.0       1.00      0.80      0.89        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.81      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.95      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.88      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n",
      "Accuracy: 0.9343626286619161\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.92      0.90      0.91       720\n",
      "         2.0       0.86      0.97      0.92       750\n",
      "         3.0       0.97      0.80      0.88       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.84      0.93      0.88       630\n",
      "         6.0       0.84      0.72      0.78       150\n",
      "         7.0       0.94      0.93      0.93       450\n",
      "         8.0       0.87      0.89      0.88       450\n",
      "         9.0       0.94      0.99      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.89      0.95      0.92       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.97      0.98       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.97      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.84      0.90        90\n",
      "        21.0       0.97      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.86      0.97      0.91       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.89      0.78      0.83       180\n",
      "        27.0       0.98      0.73      0.84        60\n",
      "        28.0       0.91      0.95      0.93       150\n",
      "        29.0       1.00      0.92      0.96        90\n",
      "        30.0       0.91      0.49      0.63       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.97      0.98      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.80      0.89        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.97      0.62      0.76        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n",
      "Accuracy: 0.936104513064133\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.92      0.91      0.92       720\n",
      "         2.0       0.87      0.98      0.92       750\n",
      "         3.0       0.98      0.81      0.89       450\n",
      "         4.0       0.93      0.96      0.94       660\n",
      "         5.0       0.86      0.94      0.90       630\n",
      "         6.0       0.85      0.70      0.77       150\n",
      "         7.0       0.94      0.93      0.93       450\n",
      "         8.0       0.88      0.90      0.89       450\n",
      "         9.0       0.95      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.94      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.98      0.99      0.99       210\n",
      "        16.0       1.00      0.96      0.98       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.91      0.93       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.86      0.91        90\n",
      "        21.0       0.98      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.88      0.77      0.82       180\n",
      "        27.0       1.00      0.77      0.87        60\n",
      "        28.0       0.90      0.96      0.93       150\n",
      "        29.0       1.00      0.97      0.98        90\n",
      "        30.0       0.87      0.46      0.60       150\n",
      "        31.0       0.90      0.99      0.94       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.99      0.98       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.83      0.91        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.93      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.94     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.94      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [500, 1000, 2000, 5000]:\n",
    "    clf_rfc_high = ensemble.RandomForestClassifier(n_estimators=i, n_jobs=8)\n",
    "    clf_rfc_high.fit(trainData, trainLabels)\n",
    "    with open('rfc_model_'+ str(i) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_rfc_high, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    predicted_labels_rfc_high = clf_rfc_high.predict(testData)\n",
    "    print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_high)))\n",
    "    print('\\n')\n",
    "    print(classification_report(testLabels, predicted_labels_rfc_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
