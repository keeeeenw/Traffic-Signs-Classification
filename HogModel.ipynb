{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import required packages\n",
    "The following code is based on https://github.com/shravankumar147/gtsrb-smai/blob/master/docs/MLP%2BClassfier%2Bon%2BHOG%2BFeatures/MLP%20Classfier%20on%20HOG%20Features.md with modificaitons for Python3 support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "from PIL import Image\n",
    "from skimage import exposure, feature, transform\n",
    "%matplotlib inline\n",
    "\n",
    "# classification required packages\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn import utils\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "# function for reading the images\n",
    "# arguments: path to the traffic sign data, for example '../../GTSRB/train/Final_Training/Images/'\n",
    "# returns: list of images, list of corresponding labels \n",
    "def readTrafficSigns(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example '../dataset/GTSRB/train/Final_Training/Images/'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(plt.imread(prefix + row[0])) # the 1th column is the filename\n",
    "            labels.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels\n",
    "\n",
    "def getAugImageLabel(imgName):\n",
    "    return int(imgName.split('_')[0])\n",
    "\n",
    "def readAugTrafficSigns(rootpath):\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    all_images = list(os.listdir(rootpath))\n",
    "    num_examples = len(all_images)\n",
    "    random.shuffle(all_images)\n",
    "    \n",
    "    for index in range(num_examples):\n",
    "        img_path = os.path.join(rootpath, all_images[index])\n",
    "        images.append(plt.imread(img_path))\n",
    "        labels.append(getAugImageLabel(all_images[index]))\n",
    "    return images, labels\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "\n",
    "    return gray\n",
    "\n",
    "def get_csv(path):\n",
    "    return [os.path.join(path,f) for f in os.listdir(path) if f.endswith('.csv')]\n",
    "\n",
    "\n",
    "def showimg_n_hog(grayimg,hogImage):\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(grayimg)\n",
    "    ax1.set_title('Input image')\n",
    "    ax1.set_adjustable('box')\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hogImage, cmap=plt.cm.gray)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    ax1.set_adjustable('box')\n",
    "    plt.show()\n",
    "    \n",
    "# Functions for testimages    \n",
    "def loadtestimages_from_path(testpath):\n",
    "    print(\"[INFO] reading all test images from directory\")\n",
    "    gtFile = get_csv(testpath)\n",
    "    filename = gtFile[0]\n",
    "    raw_data = open(filename, 'rt')\n",
    "    reader = csv.reader(raw_data, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    next(reader)\n",
    "    testfiles = list(reader)\n",
    "    timg = []\n",
    "    testimg = []\n",
    "    tlbl = []\n",
    "    for i in testfiles:\n",
    "    #     print (i[0],i[-1])\n",
    "        fname = os.path.join(testpath,i[0])\n",
    "        timg.append(fname)\n",
    "        tim = plt.imread(fname)\n",
    "        testimg.append(tim)\n",
    "        label = i[-1]\n",
    "        tlbl.append(label)\n",
    "    np.save(\"Image_n_Labels/testimagenames.npy\",timg)\n",
    "    np.save(\"Image_n_Labels/testimages.npy\",testimg)\n",
    "    np.save(\"Image_n_Labels/testimagelabels.npy\",tlbl)\n",
    "    return timg, testimg, tlbl\n",
    "    \n",
    "def loadtestimages_from_npy():\n",
    "    print(\"[INFO] loading testing images\")\n",
    "    timg = np.load(\"Image_n_Labels/testimagenames.npy\", allow_pickle=True)\n",
    "    testimg = np.load(\"Image_n_Labels/testimages.npy\", allow_pickle=True)\n",
    "    tlbl = np.load(\"Image_n_Labels/testimagelabels.npy\")\n",
    "    return timg, testimg, tlbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Official Hog Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSignsHog(rootpath, featurepath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example '../dataset/GTSRB/train/Final_Training/Images/'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    hogfeat_official = [] # images\n",
    "    hoglabels_official = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        prefix_hog = featurepath + '/' + format(c, '05d') + '/'\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            fileName = prefix_hog + row[0].rstrip(\"ppm\") + \"txt\" # swap ppm for txt extension\n",
    "            features = []\n",
    "            with open(fileName) as fp:\n",
    "                line = fp.readline()\n",
    "                while line:\n",
    "                    line = fp.readline()\n",
    "                    if len(line.strip()) > 0:\n",
    "                        features.append(float(line))\n",
    "            hogfeat_official.append(features)\n",
    "            hoglabels_official.append(row[7]) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return hogfeat_official, hoglabels_official\n",
    "\n",
    "def readTrafficSignsHogTest(testpath):\n",
    "    print(\"[INFO] reading all test hog features from directory\")\n",
    "    gtFile = get_csv(testpath)\n",
    "    filename = gtFile[0]\n",
    "    raw_data = open(filename, 'rt')\n",
    "    reader = csv.reader(raw_data, delimiter=';', quoting=csv.QUOTE_NONE)\n",
    "    next(reader)\n",
    "    testfiles = list(reader)\n",
    "    timg = []\n",
    "    testimg = []\n",
    "    tlbl = []\n",
    "    for i in testfiles:\n",
    "    #     print (i[0],i[-1])\n",
    "        fname = os.path.join(testpath,i[0].rstrip(\"ppm\") + \"txt\")\n",
    "        features = []\n",
    "        with open(fname) as fp:\n",
    "            line = fp.readline()\n",
    "            while line:\n",
    "                line = fp.readline()\n",
    "                if len(line.strip()) > 0:\n",
    "                    features.append(float(line))\n",
    "        timg.append(fname)\n",
    "        testimg.append(features)\n",
    "        label = i[-1]\n",
    "        tlbl.append(label)\n",
    "    np.save(\"Image_n_Labels/testofficialhogfeaturenames.npy\",timg)\n",
    "    np.save(\"Image_n_Labels/testofficialhogfeatures.npy\",testimg)\n",
    "    np.save(\"Image_n_Labels/testofficialhoglabels.npy\",tlbl)\n",
    "    return timg, testimg, tlbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogfeat_official, hoglabels_official = readTrafficSignsHog('../GTSRB/Final_Training/Images', '../GTSRB/Final_Training/HOG/HOG_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] reading all test hog features from directory\n"
     ]
    }
   ],
   "source": [
    "_, hogfeat_official_test, hoglabels_official_test = readTrafficSignsHogTest('../GTSRB/Final_Test/HOG/HOG_02')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training images and labels are loaded in variables ==> X,y\n",
      "[INFO] Number of training Images 39209 \n",
      "Number of Labels 39209\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Image_n_Labels/trainImages.npy\") &  os.path.isfile(\"Image_n_Labels/trainLabels.npy\") :\n",
    "    X = np.load(\"Image_n_Labels/trainImages.npy\", allow_pickle=True)\n",
    "    y = np.load(\"Image_n_Labels/trainLabels.npy\", allow_pickle=True)\n",
    "    print(\"[INFO] Training images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"[INFO] Number of training Images {} \\nNumber of Labels {}\".format(len(X), len(y)))\n",
    "else:    \n",
    "    # training images and labels\n",
    "    X, y = readTrafficSigns('../GTSRB/Final_Training/Images/')\n",
    "    np.save(\"Image_n_Labels/trainImages.npy\",X)\n",
    "    np.save(\"Image_n_Labels/trainLabels.npy\",y)\n",
    "    print(\"[INFO] training images and labels are read from the dataset directory\")\n",
    "    print(\"[INFO] training images saved to Image_n_Labels/trainingImages.npy for further use\")\n",
    "    print(\"[INFO] training labels saved to Image_n_Labels/trainingLabels.npy for further use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Augmented Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Training augmented images and labels are loaded in variables ==> X,y\n",
      "[INFO] Number of augmented training Images 45468 \n",
      "Number of Labels 45468\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(\"Image_n_Labels/trainAugImages.npy\") &  os.path.isfile(\"Image_n_Labels/trainAugLabels.npy\") :\n",
    "    trainAugImages = np.load(\"Image_n_Labels/trainAugImages.npy\", allow_pickle=True)\n",
    "    trainAugLabels = np.load(\"Image_n_Labels/trainAugLabels.npy\", allow_pickle=True)\n",
    "    print(\"[INFO] Training augmented images and labels are loaded in variables ==> X,y\")\n",
    "    print(\"[INFO] Number of augmented training Images {} \\nNumber of Labels {}\".format(len(trainAugImages), len(trainAugLabels)))\n",
    "else:    \n",
    "    # training images and labels\n",
    "    trainAugImages, trainAugLabels = readAugTrafficSigns('data/train_aug')\n",
    "    np.save(\"Image_n_Labels/trainAugImages.npy\",trainAugImages)\n",
    "    np.save(\"Image_n_Labels/trainAugLabels.npy\",trainAugLabels)\n",
    "    print(\"[INFO] training augmented images and labels are read from the dataset directory\")\n",
    "    print(\"[INFO] training augmented images saved to Image_n_Labels/trainingImages.npy for further use\")\n",
    "    print(\"[INFO] training augmented labels saved to Image_n_Labels/trainingLabels.npy for further use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1568\n"
     ]
    }
   ],
   "source": [
    "# simple test on single image for HoG features\n",
    "n = np.random.randint(0,len(X))\n",
    "i1 = X[n]\n",
    "grayim = rgb2gray(i1)\n",
    "gI1 = transform.resize(grayim,(40,40))\n",
    "# gI2 = cv2.resize(grayim, (40, 40), interpolation = cv2.INTER_CUBIC)\n",
    "# Original parms\n",
    "# (H, hogImage) = feature.hog(gI1, orientations=9, pixels_per_cell=(8,8),\n",
    "#     cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "\n",
    "# HOG 2 from http://www.lara.prd.fr/_media/users/ijcnn.pdf\n",
    "(H, hogImage) = feature.hog(gI1, orientations=8, pixels_per_cell=(5,5),\n",
    "                            cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "# hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "\n",
    "showimg_n_hog(gI1, hogImage)\n",
    "print(len(H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading testing images\n"
     ]
    }
   ],
   "source": [
    "testpath=\"../GTSRB/Final_Test/Images/\"\n",
    "if os.path.isfile(\"Image_n_Labels/testimagenames.npy\") and os.path.isfile(\"Image_n_Labels/testimages.npy\") and os.path.isfile(\"Image_n_Labels/testimagelabels.npy\"):\n",
    "    timg, testimg, tlbl = loadtestimages_from_npy()\n",
    "else:\n",
    "    timg, testimg, tlbl = loadtestimages_from_path(testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save HOG For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_save_hog(images_in, features_out, viz_out, rescale_intensity=True):\n",
    "    if os.path.isfile(features_out) and os.path.isfile(viz_out):\n",
    "        print(\"loading from file ... \")\n",
    "        hogfeat = np.load(features_out)\n",
    "        hogviz = np.load(viz_out)\n",
    "\n",
    "        print(\"HoG features are loaded\")\n",
    "        print(\"HoG visualizations are loaded\")\n",
    "    else:\n",
    "        print(features_out + \" does not found\")\n",
    "        hogfeat = []\n",
    "        hogviz = []\n",
    "        for i in range(0,len(images_in)):\n",
    "            # show an update every 1,000 images\n",
    "            if i > 0 and i % 1000 == 0:\n",
    "                print(\"[INFO] processed {}/{}\".format(i, len(images_in)))\n",
    "            I = images_in[i]\n",
    "            grayim = rgb2gray(I)\n",
    "            grayim = transform.resize(grayim,(40,40))\n",
    "\n",
    "            # TODO: try HOG2 from http://www.lara.prd.fr/_media/users/ijcnn.pdf\n",
    "            (H_5x5, hogImage) = feature.hog(grayim, orientations=8, pixels_per_cell=(5, 5),\n",
    "                cells_per_block=(2, 2), transform_sqrt=True, visualise=True)\n",
    "            if rescale_intensity:\n",
    "                hogImage = exposure.rescale_intensity(hogImage, out_range=(0, 255)).astype(\"uint8\")\n",
    "            else:\n",
    "                hogImage = hogImage.astype(\"uint8\")\n",
    "            hogviz.append(hogImage)\n",
    "            hogfeat.append(H_5x5)\n",
    "            # save the features using numpy save with .npy extention \n",
    "            # which reduced the storage space by 4times compared to pickle\n",
    "        np.save(features_out, hogfeat)\n",
    "        np.save(viz_out, hogviz)\n",
    "        print(features_out + \" are saved\")  \n",
    "        print(viz_out + \" are saved\")\n",
    "    return hogfeat, hogviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file ... \n",
      "HoG features are loaded\n",
      "HoG visualizations are loaded\n"
     ]
    }
   ],
   "source": [
    "hogfeat, hogviz = process_save_hog(X, \"HoGFeatures/HoG2features.npy\", \"HoGFeatures/HoG2visualize.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoGFeatures/HoG2featuresNoIntense.npy does not found\n",
      "[INFO] processed 1000/39209\n",
      "[INFO] processed 2000/39209\n",
      "[INFO] processed 3000/39209\n",
      "[INFO] processed 4000/39209\n",
      "[INFO] processed 5000/39209\n",
      "[INFO] processed 6000/39209\n",
      "[INFO] processed 7000/39209\n",
      "[INFO] processed 8000/39209\n",
      "[INFO] processed 9000/39209\n",
      "[INFO] processed 10000/39209\n",
      "[INFO] processed 11000/39209\n",
      "[INFO] processed 12000/39209\n",
      "[INFO] processed 13000/39209\n",
      "[INFO] processed 14000/39209\n",
      "[INFO] processed 15000/39209\n",
      "[INFO] processed 16000/39209\n",
      "[INFO] processed 17000/39209\n",
      "[INFO] processed 18000/39209\n",
      "[INFO] processed 19000/39209\n",
      "[INFO] processed 20000/39209\n",
      "[INFO] processed 21000/39209\n",
      "[INFO] processed 22000/39209\n",
      "[INFO] processed 23000/39209\n",
      "[INFO] processed 24000/39209\n",
      "[INFO] processed 25000/39209\n",
      "[INFO] processed 26000/39209\n",
      "[INFO] processed 27000/39209\n",
      "[INFO] processed 28000/39209\n",
      "[INFO] processed 29000/39209\n",
      "[INFO] processed 30000/39209\n",
      "[INFO] processed 31000/39209\n",
      "[INFO] processed 32000/39209\n",
      "[INFO] processed 33000/39209\n",
      "[INFO] processed 34000/39209\n",
      "[INFO] processed 35000/39209\n",
      "[INFO] processed 36000/39209\n",
      "[INFO] processed 37000/39209\n",
      "[INFO] processed 38000/39209\n",
      "[INFO] processed 39000/39209\n",
      "HoGFeatures/HoG2featuresNoIntense.npy are saved\n",
      "HoGFeatures/HoG2visualizeNoIntense.npy are saved\n"
     ]
    }
   ],
   "source": [
    "hogfeat_no_intense, hogviz_no_intense = process_save_hog(X, \"HoGFeatures/HoG2featuresNoIntense.npy\", \"HoGFeatures/HoG2visualizeNoIntense.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save Augmented HOG For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file ... \n",
      "HoG features are loaded\n",
      "HoG visualizations are loaded\n"
     ]
    }
   ],
   "source": [
    "hogfeat_aug, hogviz_aug = process_save_hog(trainAugImages, \"HoGFeatures/HoG2featuresAug.npy\", \"HoGFeatures/HoG2visualizeAug.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process and Save HOG For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from file ... \n",
      "HoG features are loaded\n",
      "HoG visualizations are loaded\n"
     ]
    }
   ],
   "source": [
    "hogfeat_test, hogviz_test = process_save_hog(testimg, \"HoGFeatures/HoG2features_test.npy\", \"HoGFeatures/HoG2visualize_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HoGFeatures/HoG2featuresTestNoIntense.npy does not found\n",
      "[INFO] processed 1000/12630\n",
      "[INFO] processed 2000/12630\n",
      "[INFO] processed 3000/12630\n",
      "[INFO] processed 4000/12630\n",
      "[INFO] processed 5000/12630\n",
      "[INFO] processed 6000/12630\n",
      "[INFO] processed 7000/12630\n",
      "[INFO] processed 8000/12630\n",
      "[INFO] processed 9000/12630\n",
      "[INFO] processed 10000/12630\n",
      "[INFO] processed 11000/12630\n",
      "[INFO] processed 12000/12630\n",
      "HoGFeatures/HoG2featuresTestNoIntense.npy are saved\n",
      "HoGFeatures/HoG2visualizeTestNoIntense.npy are saved\n"
     ]
    }
   ],
   "source": [
    "hogfeat_test_no_intense, hogviz_test_no_intense = process_save_hog(testimg, \"HoGFeatures/HoG2featuresTestNoIntense.npy\", \"HoGFeatures/HoG2visualizeTestNoIntense.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainData = np.array(hogfeat).astype(\"float\")\n",
    "trainLabels = y.astype(\"float\")\n",
    "\n",
    "# Testing\n",
    "testData = np.array(hogfeat_test).astype(\"float\")\n",
    "testLabels = np.array(tlbl).astype(\"float\")\n",
    "\n",
    "# (trainDataShuffled, _, trainLabelsShuffled, _) = train_test_split(trainData, trainLabels,\n",
    "#      test_size=None, random_state=84)\n",
    "\n",
    "(trainDataShuffled, trainLabelsShuffled) = utils.shuffle(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39209, 1568)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29406, 1568)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataShuffled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = neighbors.KNeighborsClassifier()\n",
    "clf.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = svm.SVC()\n",
    "clf_svm.fit(trainData, trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = False\n",
    "if save:\n",
    "    with open('knn_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    with open('svm_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_svm, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    with open('svm_linear_model.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_svm_linear, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "else:\n",
    "    with open('knn_model.pickle', 'rb') as handle:\n",
    "        clf = pickle.load(handle)\n",
    "    \n",
    "    with open('svm_model.pickle', 'rb') as handle:\n",
    "        clf_svm = pickle.load(handle)\n",
    "        \n",
    "    with open('svm_linear_model.pickle', 'rb') as handle:\n",
    "        clf_svm_linear = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear = svm.LinearSVC(max_iter=10000)\n",
    "clf_svm_linear.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear2 = svm.LinearSVC(max_iter=100000)\n",
    "clf_svm_linear2.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_2.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear3 = svm.LinearSVC(max_iter=10000, loss='hinge')\n",
    "clf_svm_linear3.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_3.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear4 = svm.LinearSVC(max_iter=10000, tol=1e-5)\n",
    "clf_svm_linear4.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_4.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_linear5 = svm.LinearSVC(max_iter=100000, tol=1e-5)\n",
    "clf_svm_linear5.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_5.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear5, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_rfc = ensemble.RandomForestClassifier()\n",
    "clf_rfc.fit(trainData, trainLabels)\n",
    "with open('rfc_model.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc2 = ensemble.RandomForestClassifier(n_estimators=100)\n",
    "clf_rfc2.fit(trainData, trainLabels)\n",
    "with open('rfc_model2.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc2, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc3 = ensemble.RandomForestClassifier(n_estimators=200)\n",
    "clf_rfc3.fit(trainData, trainLabels)\n",
    "with open('rfc_model3.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc3, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc4 = ensemble.RandomForestClassifier(n_estimators=500)\n",
    "clf_rfc4.fit(trainData, trainLabels)\n",
    "with open('rfc_model4.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc4, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc5 = ensemble.RandomForestClassifier(min_samples_split=500, n_estimators=500, n_jobs=8)\n",
    "clf_rfc5.fit(trainData, trainLabels)\n",
    "with open('rfc_model5.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc5, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rfc6 = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=8)\n",
    "clf_rfc6.fit(trainData, trainLabels)\n",
    "with open('rfc_model6.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc6, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With HOG2\n",
    "clf_rfc7 = ensemble.RandomForestClassifier(n_estimators=500, n_jobs=8)\n",
    "clf_rfc7.fit(trainData, trainLabels)\n",
    "with open('rfc_model7.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc7, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = clf.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7969912905779889\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.72      0.38      0.50        60\n",
      "         1.0       0.78      0.64      0.70       720\n",
      "         2.0       0.70      0.70      0.70       750\n",
      "         3.0       0.59      0.66      0.62       450\n",
      "         4.0       0.77      0.90      0.83       660\n",
      "         5.0       0.48      0.67      0.56       630\n",
      "         6.0       0.89      0.73      0.80       150\n",
      "         7.0       0.73      0.69      0.71       450\n",
      "         8.0       0.66      0.70      0.68       450\n",
      "         9.0       0.92      0.78      0.84       480\n",
      "        10.0       0.84      0.93      0.88       660\n",
      "        11.0       0.79      0.75      0.77       420\n",
      "        12.0       1.00      1.00      1.00       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.91      0.95       270\n",
      "        15.0       0.99      0.95      0.97       210\n",
      "        16.0       0.91      0.98      0.94       150\n",
      "        17.0       1.00      0.94      0.97       360\n",
      "        18.0       0.82      0.74      0.78       390\n",
      "        19.0       0.68      0.83      0.75        60\n",
      "        20.0       0.88      0.76      0.81        90\n",
      "        21.0       0.66      0.64      0.65        90\n",
      "        22.0       0.59      0.69      0.64       120\n",
      "        23.0       0.62      0.54      0.58       150\n",
      "        24.0       0.62      0.66      0.64        90\n",
      "        25.0       0.75      0.72      0.74       480\n",
      "        26.0       0.51      0.68      0.58       180\n",
      "        27.0       0.67      0.80      0.73        60\n",
      "        28.0       0.52      0.41      0.46       150\n",
      "        29.0       0.60      0.50      0.55        90\n",
      "        30.0       0.56      0.25      0.35       150\n",
      "        31.0       0.65      0.97      0.78       270\n",
      "        32.0       0.97      0.95      0.96        60\n",
      "        33.0       0.99      0.94      0.96       210\n",
      "        34.0       0.96      0.84      0.90       120\n",
      "        35.0       1.00      0.93      0.96       390\n",
      "        36.0       0.98      0.95      0.97       120\n",
      "        37.0       1.00      0.60      0.75        60\n",
      "        38.0       1.00      0.91      0.95       690\n",
      "        39.0       1.00      0.97      0.98        90\n",
      "        40.0       0.99      0.80      0.88        90\n",
      "        41.0       0.86      0.60      0.71        60\n",
      "        42.0       0.85      0.76      0.80        90\n",
      "\n",
      "    accuracy                           0.80     12630\n",
      "   macro avg       0.80      0.76      0.77     12630\n",
      "weighted avg       0.81      0.80      0.80     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_svm = clf_svm.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7781472684085511\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.72      0.73      0.72       720\n",
      "         2.0       0.51      0.96      0.67       750\n",
      "         3.0       0.98      0.36      0.53       450\n",
      "         4.0       0.92      0.94      0.93       660\n",
      "         5.0       0.66      0.63      0.64       630\n",
      "         6.0       0.66      0.65      0.65       150\n",
      "         7.0       0.93      0.80      0.86       450\n",
      "         8.0       0.86      0.86      0.86       450\n",
      "         9.0       0.91      0.89      0.90       480\n",
      "        10.0       0.87      0.95      0.91       660\n",
      "        11.0       0.52      0.94      0.67       420\n",
      "        12.0       0.98      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.87      0.93       270\n",
      "        15.0       0.99      0.91      0.95       210\n",
      "        16.0       1.00      0.67      0.80       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.66      0.83      0.74       390\n",
      "        19.0       0.00      0.00      0.00        60\n",
      "        20.0       0.00      0.00      0.00        90\n",
      "        21.0       0.00      0.00      0.00        90\n",
      "        22.0       0.00      0.00      0.00       120\n",
      "        23.0       1.00      0.26      0.41       150\n",
      "        24.0       0.00      0.00      0.00        90\n",
      "        25.0       0.46      1.00      0.63       480\n",
      "        26.0       0.00      0.00      0.00       180\n",
      "        27.0       0.00      0.00      0.00        60\n",
      "        28.0       1.00      0.01      0.01       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       1.00      0.03      0.06       150\n",
      "        31.0       0.60      0.97      0.74       270\n",
      "        32.0       1.00      0.50      0.67        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       1.00      0.93      0.97       120\n",
      "        35.0       0.93      0.97      0.95       390\n",
      "        36.0       1.00      0.94      0.97       120\n",
      "        37.0       1.00      0.27      0.42        60\n",
      "        38.0       0.96      1.00      0.98       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       1.00      0.56      0.71        90\n",
      "        41.0       1.00      0.23      0.38        60\n",
      "        42.0       1.00      0.01      0.02        90\n",
      "\n",
      "    accuracy                           0.78     12630\n",
      "   macro avg       0.70      0.57      0.57     12630\n",
      "weighted avg       0.78      0.78      0.74     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_svm_linear = clf_svm_linear.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear2 = clf_svm_linear2.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear2)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9208234362628662\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.90      0.86      0.88       720\n",
      "         2.0       0.85      0.93      0.89       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.82      0.81      0.82       630\n",
      "         6.0       0.83      0.77      0.80       150\n",
      "         7.0       0.94      0.91      0.92       450\n",
      "         8.0       0.84      0.85      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.90      0.92       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.90      1.00      0.95       210\n",
      "        16.0       0.95      0.99      0.97       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.90      0.94       390\n",
      "        19.0       0.92      1.00      0.96        60\n",
      "        20.0       0.90      0.89      0.89        90\n",
      "        21.0       0.93      0.78      0.85        90\n",
      "        22.0       1.00      0.79      0.88       120\n",
      "        23.0       0.82      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.90      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.93      0.93      0.93        60\n",
      "        28.0       0.84      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.80      0.55      0.65       150\n",
      "        31.0       0.89      0.98      0.93       270\n",
      "        32.0       0.84      0.78      0.81        60\n",
      "        33.0       0.95      1.00      0.97       210\n",
      "        34.0       0.93      0.99      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.95      1.00      0.98       120\n",
      "        37.0       1.00      0.90      0.95        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.87      0.98      0.92        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.84      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear3 = clf_svm_linear3.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear3)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear4 = clf_svm_linear4.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear4)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211401425178147\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.70      0.75        60\n",
      "         1.0       0.88      0.86      0.87       720\n",
      "         2.0       0.84      0.92      0.88       750\n",
      "         3.0       0.87      0.85      0.86       450\n",
      "         4.0       0.94      0.93      0.94       660\n",
      "         5.0       0.80      0.81      0.81       630\n",
      "         6.0       0.84      0.76      0.80       150\n",
      "         7.0       0.94      0.89      0.92       450\n",
      "         8.0       0.85      0.86      0.85       450\n",
      "         9.0       0.96      0.98      0.97       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.93      0.91      0.92       420\n",
      "        12.0       0.98      0.98      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.97      0.97      0.97       270\n",
      "        15.0       0.92      1.00      0.96       210\n",
      "        16.0       0.94      0.98      0.96       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.89      0.94       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.91      0.90      0.91        90\n",
      "        21.0       0.93      0.79      0.86        90\n",
      "        22.0       0.99      0.79      0.88       120\n",
      "        23.0       0.83      0.89      0.86       150\n",
      "        24.0       0.95      0.92      0.94        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.86      0.79      0.82       180\n",
      "        27.0       0.92      0.93      0.93        60\n",
      "        28.0       0.85      0.96      0.90       150\n",
      "        29.0       0.80      0.91      0.85        90\n",
      "        30.0       0.81      0.57      0.67       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.84      0.80      0.82        60\n",
      "        33.0       0.96      0.99      0.97       210\n",
      "        34.0       0.94      0.98      0.96       120\n",
      "        35.0       0.99      0.97      0.98       390\n",
      "        36.0       0.96      1.00      0.98       120\n",
      "        37.0       1.00      0.93      0.97        60\n",
      "        38.0       0.97      0.98      0.97       690\n",
      "        39.0       0.91      0.94      0.93        90\n",
      "        40.0       0.93      0.98      0.95        90\n",
      "        41.0       0.94      0.83      0.88        60\n",
      "        42.0       0.85      0.78      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.90      0.91     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_svm_linear5 = clf_svm_linear5.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_linear5)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_linear5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8401425178147268\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.66      0.58      0.62        60\n",
      "         1.0       0.71      0.82      0.76       720\n",
      "         2.0       0.68      0.89      0.77       750\n",
      "         3.0       0.79      0.73      0.76       450\n",
      "         4.0       0.86      0.91      0.89       660\n",
      "         5.0       0.65      0.67      0.66       630\n",
      "         6.0       0.85      0.61      0.71       150\n",
      "         7.0       0.83      0.84      0.84       450\n",
      "         8.0       0.77      0.72      0.74       450\n",
      "         9.0       0.87      0.95      0.91       480\n",
      "        10.0       0.93      0.93      0.93       660\n",
      "        11.0       0.76      0.89      0.82       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.98      0.99      0.99       720\n",
      "        14.0       0.95      0.87      0.91       270\n",
      "        15.0       0.95      0.91      0.93       210\n",
      "        16.0       0.99      0.89      0.94       150\n",
      "        17.0       0.99      0.95      0.97       360\n",
      "        18.0       0.84      0.84      0.84       390\n",
      "        19.0       0.77      0.92      0.84        60\n",
      "        20.0       0.78      0.80      0.79        90\n",
      "        21.0       0.69      0.49      0.57        90\n",
      "        22.0       0.92      0.73      0.81       120\n",
      "        23.0       0.72      0.85      0.78       150\n",
      "        24.0       0.84      0.69      0.76        90\n",
      "        25.0       0.81      0.87      0.84       480\n",
      "        26.0       0.76      0.70      0.73       180\n",
      "        27.0       0.76      0.37      0.49        60\n",
      "        28.0       0.72      0.69      0.71       150\n",
      "        29.0       0.61      0.38      0.47        90\n",
      "        30.0       0.51      0.25      0.33       150\n",
      "        31.0       0.90      0.89      0.89       270\n",
      "        32.0       0.93      0.68      0.79        60\n",
      "        33.0       0.93      0.91      0.92       210\n",
      "        34.0       0.92      0.97      0.94       120\n",
      "        35.0       0.96      0.91      0.94       390\n",
      "        36.0       0.92      0.84      0.88       120\n",
      "        37.0       0.98      0.68      0.80        60\n",
      "        38.0       0.95      0.93      0.94       690\n",
      "        39.0       0.96      0.87      0.91        90\n",
      "        40.0       0.92      0.50      0.65        90\n",
      "        41.0       0.82      0.47      0.60        60\n",
      "        42.0       0.96      0.52      0.68        90\n",
      "\n",
      "    accuracy                           0.84     12630\n",
      "   macro avg       0.84      0.76      0.79     12630\n",
      "weighted avg       0.84      0.84      0.84     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc = clf_rfc.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9219319081551861\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.91      0.89      0.90       720\n",
      "         2.0       0.82      0.96      0.89       750\n",
      "         3.0       0.94      0.80      0.86       450\n",
      "         4.0       0.91      0.95      0.93       660\n",
      "         5.0       0.79      0.87      0.83       630\n",
      "         6.0       0.88      0.71      0.79       150\n",
      "         7.0       0.92      0.92      0.92       450\n",
      "         8.0       0.88      0.87      0.88       450\n",
      "         9.0       0.94      0.99      0.96       480\n",
      "        10.0       0.97      0.98      0.98       660\n",
      "        11.0       0.86      0.94      0.90       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.94      0.97       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.93      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.90      0.93       390\n",
      "        19.0       0.95      0.98      0.97        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       0.98      0.72      0.83        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.82      0.94      0.88       150\n",
      "        24.0       0.99      0.87      0.92        90\n",
      "        25.0       0.82      0.98      0.89       480\n",
      "        26.0       0.85      0.76      0.80       180\n",
      "        27.0       1.00      0.58      0.74        60\n",
      "        28.0       0.85      0.95      0.90       150\n",
      "        29.0       0.99      0.83      0.90        90\n",
      "        30.0       0.86      0.45      0.59       150\n",
      "        31.0       0.92      0.98      0.95       270\n",
      "        32.0       0.98      0.95      0.97        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.97      0.97       390\n",
      "        36.0       1.00      0.96      0.98       120\n",
      "        37.0       1.00      0.70      0.82        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.97      0.82      0.89        90\n",
      "        41.0       0.97      0.58      0.73        60\n",
      "        42.0       0.95      0.63      0.76        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.87      0.90     12630\n",
      "weighted avg       0.93      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc2 = clf_rfc2.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc2)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9287410926365796\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.90      0.91      0.91       720\n",
      "         2.0       0.84      0.97      0.90       750\n",
      "         3.0       0.96      0.79      0.87       450\n",
      "         4.0       0.91      0.95      0.93       660\n",
      "         5.0       0.82      0.90      0.86       630\n",
      "         6.0       0.88      0.71      0.79       150\n",
      "         7.0       0.93      0.93      0.93       450\n",
      "         8.0       0.89      0.88      0.88       450\n",
      "         9.0       0.94      0.99      0.96       480\n",
      "        10.0       0.96      0.99      0.97       660\n",
      "        11.0       0.85      0.94      0.89       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.97      0.98       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.94      0.97       150\n",
      "        17.0       1.00      0.99      0.99       360\n",
      "        18.0       0.96      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       1.00      0.83      0.91        90\n",
      "        21.0       0.97      0.71      0.82        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.91      0.96      0.93       150\n",
      "        24.0       1.00      0.88      0.93        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.90      0.78      0.83       180\n",
      "        27.0       0.98      0.68      0.80        60\n",
      "        28.0       0.86      0.97      0.91       150\n",
      "        29.0       1.00      0.87      0.93        90\n",
      "        30.0       0.90      0.53      0.66       150\n",
      "        31.0       0.92      0.98      0.95       270\n",
      "        32.0       0.98      0.97      0.97        60\n",
      "        33.0       0.99      1.00      1.00       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.97      0.97       390\n",
      "        36.0       1.00      0.95      0.97       120\n",
      "        37.0       1.00      0.70      0.82        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.81      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.95      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.88      0.91     12630\n",
      "weighted avg       0.93      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc3 = clf_rfc3.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc3)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9323832145684877\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.57      0.72        60\n",
      "         1.0       0.90      0.90      0.90       720\n",
      "         2.0       0.86      0.97      0.91       750\n",
      "         3.0       0.98      0.81      0.89       450\n",
      "         4.0       0.92      0.96      0.94       660\n",
      "         5.0       0.83      0.92      0.87       630\n",
      "         6.0       0.84      0.69      0.76       150\n",
      "         7.0       0.93      0.94      0.94       450\n",
      "         8.0       0.90      0.89      0.89       450\n",
      "         9.0       0.95      0.99      0.97       480\n",
      "        10.0       0.96      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.98      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.97      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.98       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.97      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       0.98      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.87      0.95      0.91       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.87      0.78      0.82       180\n",
      "        27.0       1.00      0.80      0.89        60\n",
      "        28.0       0.91      0.95      0.93       150\n",
      "        29.0       1.00      0.93      0.97        90\n",
      "        30.0       0.90      0.49      0.63       150\n",
      "        31.0       0.90      0.98      0.94       270\n",
      "        32.0       1.00      0.98      0.99        60\n",
      "        33.0       0.97      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.95      0.97       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.96      0.61      0.75        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc4 = clf_rfc4.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc4)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8468725257323833\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.80      0.81      0.81       720\n",
      "         2.0       0.76      0.95      0.84       750\n",
      "         3.0       0.88      0.79      0.84       450\n",
      "         4.0       0.82      0.92      0.87       660\n",
      "         5.0       0.75      0.77      0.76       630\n",
      "         6.0       0.92      0.47      0.62       150\n",
      "         7.0       0.81      0.93      0.86       450\n",
      "         8.0       0.81      0.78      0.79       450\n",
      "         9.0       0.83      0.97      0.89       480\n",
      "        10.0       0.89      0.98      0.93       660\n",
      "        11.0       0.70      0.93      0.80       420\n",
      "        12.0       0.96      1.00      0.98       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.99      0.89      0.93       270\n",
      "        15.0       0.96      0.99      0.97       210\n",
      "        16.0       1.00      0.79      0.88       150\n",
      "        17.0       1.00      0.96      0.98       360\n",
      "        18.0       0.81      0.90      0.85       390\n",
      "        19.0       1.00      0.15      0.26        60\n",
      "        20.0       0.99      0.77      0.86        90\n",
      "        21.0       1.00      0.19      0.32        90\n",
      "        22.0       0.97      0.72      0.83       120\n",
      "        23.0       0.70      0.77      0.73       150\n",
      "        24.0       1.00      0.01      0.02        90\n",
      "        25.0       0.63      0.97      0.77       480\n",
      "        26.0       0.83      0.68      0.75       180\n",
      "        27.0       1.00      0.07      0.12        60\n",
      "        28.0       0.93      0.63      0.75       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.79      0.22      0.34       150\n",
      "        31.0       0.73      0.97      0.83       270\n",
      "        32.0       1.00      0.50      0.67        60\n",
      "        33.0       0.97      0.97      0.97       210\n",
      "        34.0       0.98      0.93      0.96       120\n",
      "        35.0       0.92      0.98      0.95       390\n",
      "        36.0       1.00      0.91      0.95       120\n",
      "        37.0       1.00      0.38      0.55        60\n",
      "        38.0       0.93      0.98      0.96       690\n",
      "        39.0       1.00      0.91      0.95        90\n",
      "        40.0       1.00      0.16      0.27        90\n",
      "        41.0       0.00      0.00      0.00        60\n",
      "        42.0       0.97      0.36      0.52        90\n",
      "\n",
      "    accuracy                           0.85     12630\n",
      "   macro avg       0.84      0.68      0.70     12630\n",
      "weighted avg       0.85      0.85      0.83     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc5 = clf_rfc5.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc5)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9338083927157561\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        60\n",
      "         1.0       0.90      0.91      0.91       720\n",
      "         2.0       0.86      0.98      0.91       750\n",
      "         3.0       0.97      0.80      0.87       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.85      0.93      0.88       630\n",
      "         6.0       0.85      0.70      0.77       150\n",
      "         7.0       0.93      0.92      0.93       450\n",
      "         8.0       0.88      0.90      0.89       450\n",
      "         9.0       0.94      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.94      0.97       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.99      0.86      0.92        90\n",
      "        21.0       0.98      0.70      0.82        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.89      0.79      0.84       180\n",
      "        27.0       1.00      0.73      0.85        60\n",
      "        28.0       0.91      0.97      0.94       150\n",
      "        29.0       1.00      0.90      0.95        90\n",
      "        30.0       0.87      0.49      0.63       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.98      0.98      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.99      0.99       120\n",
      "        35.0       0.97      0.98      0.97       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.96      0.61      0.75        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc6 = clf_rfc6.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc6)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9347585114806017\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        60\n",
      "         1.0       0.91      0.91      0.91       720\n",
      "         2.0       0.87      0.97      0.92       750\n",
      "         3.0       0.97      0.81      0.88       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.84      0.93      0.88       630\n",
      "         6.0       0.86      0.73      0.79       150\n",
      "         7.0       0.94      0.94      0.94       450\n",
      "         8.0       0.88      0.89      0.88       450\n",
      "         9.0       0.94      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.89      0.94      0.92       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.96      0.91      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.99      0.84      0.91        90\n",
      "        21.0       0.97      0.70      0.81        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.91      0.95        90\n",
      "        25.0       0.83      0.98      0.90       480\n",
      "        26.0       0.88      0.78      0.82       180\n",
      "        27.0       1.00      0.77      0.87        60\n",
      "        28.0       0.88      0.97      0.93       150\n",
      "        29.0       0.99      0.88      0.93        90\n",
      "        30.0       0.87      0.51      0.64       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.97      1.00      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.99      0.98      0.99       120\n",
      "        35.0       0.97      0.98      0.97       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.82      0.90        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       0.97      0.62      0.76        60\n",
      "        42.0       0.97      0.62      0.76        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels_rfc7 = clf_rfc7.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc7)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_rfc7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9335708630245447\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.91      0.91      0.91       720\n",
      "         2.0       0.85      0.98      0.91       750\n",
      "         3.0       0.97      0.81      0.88       450\n",
      "         4.0       0.93      0.96      0.94       660\n",
      "         5.0       0.85      0.91      0.88       630\n",
      "         6.0       0.83      0.71      0.77       150\n",
      "         7.0       0.92      0.94      0.93       450\n",
      "         8.0       0.88      0.88      0.88       450\n",
      "         9.0       0.95      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.95      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.94      0.97       270\n",
      "        15.0       0.98      0.99      0.99       210\n",
      "        16.0       1.00      0.96      0.98       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.97      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.86      0.91        90\n",
      "        21.0       0.98      0.69      0.81        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.83      0.97      0.90       150\n",
      "        24.0       1.00      0.91      0.95        90\n",
      "        25.0       0.85      0.98      0.91       480\n",
      "        26.0       0.89      0.79      0.84       180\n",
      "        27.0       1.00      0.63      0.78        60\n",
      "        28.0       0.91      0.96      0.93       150\n",
      "        29.0       1.00      0.94      0.97        90\n",
      "        30.0       0.90      0.49      0.64       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      1.00      0.99       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.97      0.98       120\n",
      "        37.0       1.00      0.80      0.89        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.81      0.90        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.95      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.88      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n",
      "Accuracy: 0.9343626286619161\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.92      0.90      0.91       720\n",
      "         2.0       0.86      0.97      0.92       750\n",
      "         3.0       0.97      0.80      0.88       450\n",
      "         4.0       0.93      0.96      0.95       660\n",
      "         5.0       0.84      0.93      0.88       630\n",
      "         6.0       0.84      0.72      0.78       150\n",
      "         7.0       0.94      0.93      0.93       450\n",
      "         8.0       0.87      0.89      0.88       450\n",
      "         9.0       0.94      0.99      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.89      0.95      0.92       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.97      0.98       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.97      0.92      0.94       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.84      0.90        90\n",
      "        21.0       0.97      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.86      0.97      0.91       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.89      0.78      0.83       180\n",
      "        27.0       0.98      0.73      0.84        60\n",
      "        28.0       0.91      0.95      0.93       150\n",
      "        29.0       1.00      0.92      0.96        90\n",
      "        30.0       0.91      0.49      0.63       150\n",
      "        31.0       0.91      0.99      0.95       270\n",
      "        32.0       0.97      0.98      0.98        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       1.00      0.97      0.99       120\n",
      "        37.0       1.00      0.80      0.89        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.82      0.90        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.97      0.62      0.76        90\n",
      "\n",
      "    accuracy                           0.93     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.93      0.93     12630\n",
      "\n",
      "Accuracy: 0.936104513064133\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.92      0.91      0.92       720\n",
      "         2.0       0.87      0.98      0.92       750\n",
      "         3.0       0.98      0.81      0.89       450\n",
      "         4.0       0.93      0.96      0.94       660\n",
      "         5.0       0.86      0.94      0.90       630\n",
      "         6.0       0.85      0.70      0.77       150\n",
      "         7.0       0.94      0.93      0.93       450\n",
      "         8.0       0.88      0.90      0.89       450\n",
      "         9.0       0.95      1.00      0.97       480\n",
      "        10.0       0.97      0.99      0.98       660\n",
      "        11.0       0.88      0.94      0.91       420\n",
      "        12.0       0.99      1.00      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.96      0.98       270\n",
      "        15.0       0.98      0.99      0.99       210\n",
      "        16.0       1.00      0.96      0.98       150\n",
      "        17.0       1.00      1.00      1.00       360\n",
      "        18.0       0.96      0.91      0.93       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.86      0.91        90\n",
      "        21.0       0.98      0.68      0.80        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.90      0.95        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.88      0.77      0.82       180\n",
      "        27.0       1.00      0.77      0.87        60\n",
      "        28.0       0.90      0.96      0.93       150\n",
      "        29.0       1.00      0.97      0.98        90\n",
      "        30.0       0.87      0.46      0.60       150\n",
      "        31.0       0.90      0.99      0.94       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.98      0.98      0.98       120\n",
      "        35.0       0.97      0.99      0.98       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.83      0.91        90\n",
      "        41.0       1.00      0.62      0.76        60\n",
      "        42.0       0.93      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.94     12630\n",
      "   macro avg       0.95      0.89      0.91     12630\n",
      "weighted avg       0.94      0.94      0.93     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [500, 1000, 2000, 5000]:\n",
    "    clf_rfc_high = ensemble.RandomForestClassifier(n_estimators=i, n_jobs=8)\n",
    "    clf_rfc_high.fit(trainData, trainLabels)\n",
    "    with open('rfc_model_'+ str(i) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_rfc_high, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    predicted_labels_rfc_high = clf_rfc_high.predict(testData)\n",
    "    print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_high)))\n",
    "    print('\\n')\n",
    "    print(classification_report(testLabels, predicted_labels_rfc_high))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 1000 min_samples_split: 10 max_features: 100\n",
      "Accuracy: 0.9121931908155186\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.73      0.85        60\n",
      "         1.0       0.90      0.80      0.85       720\n",
      "         2.0       0.80      0.96      0.87       750\n",
      "         3.0       0.95      0.78      0.86       450\n",
      "         4.0       0.93      0.94      0.93       660\n",
      "         5.0       0.82      0.86      0.84       630\n",
      "         6.0       0.82      0.73      0.77       150\n",
      "         7.0       0.90      0.92      0.91       450\n",
      "         8.0       0.86      0.87      0.86       450\n",
      "         9.0       0.90      0.98      0.93       480\n",
      "        10.0       0.92      0.98      0.95       660\n",
      "        11.0       0.88      0.89      0.89       420\n",
      "        12.0       0.99      0.99      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.95      0.96       270\n",
      "        15.0       0.98      0.97      0.97       210\n",
      "        16.0       0.94      0.97      0.95       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.89      0.88      0.89       390\n",
      "        19.0       0.98      0.98      0.98        60\n",
      "        20.0       0.93      0.87      0.90        90\n",
      "        21.0       1.00      0.66      0.79        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.89      0.94      0.91       150\n",
      "        24.0       1.00      0.81      0.90        90\n",
      "        25.0       0.84      0.95      0.90       480\n",
      "        26.0       0.84      0.76      0.80       180\n",
      "        27.0       0.91      0.72      0.80        60\n",
      "        28.0       0.84      0.89      0.86       150\n",
      "        29.0       0.95      0.84      0.89        90\n",
      "        30.0       0.83      0.70      0.76       150\n",
      "        31.0       0.89      0.98      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.98       210\n",
      "        34.0       1.00      0.98      0.99       120\n",
      "        35.0       0.95      0.98      0.96       390\n",
      "        36.0       0.98      0.97      0.97       120\n",
      "        37.0       1.00      0.80      0.89        60\n",
      "        38.0       0.94      0.99      0.96       690\n",
      "        39.0       0.99      0.98      0.98        90\n",
      "        40.0       0.99      0.80      0.88        90\n",
      "        41.0       0.97      0.62      0.76        60\n",
      "        42.0       0.91      0.64      0.75        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.88      0.90     12630\n",
      "weighted avg       0.91      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.9140142517814727\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80        60\n",
      "         1.0       0.91      0.80      0.85       720\n",
      "         2.0       0.81      0.96      0.88       750\n",
      "         3.0       0.95      0.79      0.86       450\n",
      "         4.0       0.92      0.94      0.93       660\n",
      "         5.0       0.82      0.86      0.84       630\n",
      "         6.0       0.83      0.73      0.78       150\n",
      "         7.0       0.89      0.91      0.90       450\n",
      "         8.0       0.85      0.86      0.86       450\n",
      "         9.0       0.89      0.98      0.93       480\n",
      "        10.0       0.92      0.98      0.95       660\n",
      "        11.0       0.89      0.91      0.90       420\n",
      "        12.0       0.98      0.99      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.99      0.96      0.97       270\n",
      "        15.0       0.98      0.98      0.98       210\n",
      "        16.0       0.95      0.97      0.96       150\n",
      "        17.0       0.99      0.95      0.97       360\n",
      "        18.0       0.90      0.88      0.89       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.92      0.87      0.89        90\n",
      "        21.0       1.00      0.64      0.78        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.85      0.97      0.91       150\n",
      "        24.0       1.00      0.82      0.90        90\n",
      "        25.0       0.84      0.96      0.90       480\n",
      "        26.0       0.84      0.76      0.80       180\n",
      "        27.0       0.93      0.68      0.79        60\n",
      "        28.0       0.85      0.89      0.87       150\n",
      "        29.0       0.96      0.87      0.91        90\n",
      "        30.0       0.90      0.67      0.77       150\n",
      "        31.0       0.90      0.98      0.94       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.98      0.98      0.98       210\n",
      "        34.0       1.00      1.00      1.00       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.98      0.97      0.97       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       0.99      0.98      0.98        90\n",
      "        40.0       0.99      0.78      0.87        90\n",
      "        41.0       0.95      0.58      0.72        60\n",
      "        42.0       0.94      0.67      0.78        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.88      0.90     12630\n",
      "weighted avg       0.92      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.9182106096595408\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.55      0.71        60\n",
      "         1.0       0.91      0.81      0.86       720\n",
      "         2.0       0.80      0.97      0.88       750\n",
      "         3.0       0.97      0.79      0.87       450\n",
      "         4.0       0.92      0.93      0.93       660\n",
      "         5.0       0.83      0.86      0.84       630\n",
      "         6.0       0.86      0.75      0.80       150\n",
      "         7.0       0.90      0.92      0.91       450\n",
      "         8.0       0.87      0.88      0.87       450\n",
      "         9.0       0.90      0.98      0.94       480\n",
      "        10.0       0.94      0.99      0.96       660\n",
      "        11.0       0.88      0.92      0.90       420\n",
      "        12.0       0.98      0.99      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.95      0.97       270\n",
      "        15.0       0.99      0.98      0.99       210\n",
      "        16.0       0.99      0.97      0.98       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.91      0.89      0.90       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.96      0.87      0.91        90\n",
      "        21.0       1.00      0.63      0.78        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.89      0.95      0.92       150\n",
      "        24.0       1.00      0.82      0.90        90\n",
      "        25.0       0.83      0.97      0.90       480\n",
      "        26.0       0.85      0.77      0.81       180\n",
      "        27.0       0.96      0.73      0.83        60\n",
      "        28.0       0.85      0.89      0.87       150\n",
      "        29.0       1.00      0.89      0.94        90\n",
      "        30.0       0.92      0.65      0.76       150\n",
      "        31.0       0.89      0.98      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.98       210\n",
      "        34.0       1.00      0.99      1.00       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.99      0.96      0.97       120\n",
      "        37.0       0.98      0.78      0.87        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.99      0.80      0.88        90\n",
      "        41.0       1.00      0.57      0.72        60\n",
      "        42.0       0.93      0.71      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.88      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.9063341250989707\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.27      0.42        60\n",
      "         1.0       0.91      0.78      0.84       720\n",
      "         2.0       0.74      0.97      0.84       750\n",
      "         3.0       0.98      0.73      0.83       450\n",
      "         4.0       0.90      0.95      0.92       660\n",
      "         5.0       0.85      0.85      0.85       630\n",
      "         6.0       0.86      0.75      0.80       150\n",
      "         7.0       0.91      0.92      0.92       450\n",
      "         8.0       0.86      0.89      0.87       450\n",
      "         9.0       0.92      0.95      0.94       480\n",
      "        10.0       0.93      0.99      0.96       660\n",
      "        11.0       0.83      0.94      0.88       420\n",
      "        12.0       0.96      0.99      0.98       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.95      0.97       270\n",
      "        15.0       1.00      0.98      0.99       210\n",
      "        16.0       1.00      0.95      0.97       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.88      0.93      0.91       390\n",
      "        19.0       1.00      0.95      0.97        60\n",
      "        20.0       1.00      0.81      0.90        90\n",
      "        21.0       1.00      0.61      0.76        90\n",
      "        22.0       1.00      0.74      0.85       120\n",
      "        23.0       0.89      0.86      0.87       150\n",
      "        24.0       1.00      0.66      0.79        90\n",
      "        25.0       0.75      0.99      0.85       480\n",
      "        26.0       0.92      0.72      0.81       180\n",
      "        27.0       1.00      0.53      0.70        60\n",
      "        28.0       0.89      0.84      0.87       150\n",
      "        29.0       1.00      0.83      0.91        90\n",
      "        30.0       0.98      0.41      0.58       150\n",
      "        31.0       0.83      0.98      0.90       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.99      0.98       210\n",
      "        34.0       1.00      0.96      0.98       120\n",
      "        35.0       0.96      0.99      0.98       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.95      1.00      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.79      0.88        90\n",
      "        41.0       0.97      0.52      0.67        60\n",
      "        42.0       0.95      0.68      0.79        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.94      0.84      0.88     12630\n",
      "weighted avg       0.92      0.91      0.90     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8850356294536817\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.60      0.75        60\n",
      "         1.0       0.87      0.75      0.81       720\n",
      "         2.0       0.78      0.95      0.86       750\n",
      "         3.0       0.93      0.76      0.83       450\n",
      "         4.0       0.90      0.91      0.90       660\n",
      "         5.0       0.76      0.79      0.78       630\n",
      "         6.0       0.84      0.63      0.72       150\n",
      "         7.0       0.85      0.88      0.87       450\n",
      "         8.0       0.78      0.85      0.82       450\n",
      "         9.0       0.88      0.97      0.92       480\n",
      "        10.0       0.88      0.96      0.92       660\n",
      "        11.0       0.88      0.87      0.87       420\n",
      "        12.0       0.97      0.98      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.93      0.96       270\n",
      "        15.0       0.96      0.96      0.96       210\n",
      "        16.0       0.87      0.93      0.90       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.82      0.87      0.85       390\n",
      "        19.0       0.98      0.97      0.97        60\n",
      "        20.0       0.92      0.87      0.89        90\n",
      "        21.0       0.98      0.58      0.73        90\n",
      "        22.0       0.99      0.73      0.84       120\n",
      "        23.0       0.86      0.92      0.89       150\n",
      "        24.0       0.98      0.60      0.74        90\n",
      "        25.0       0.81      0.94      0.87       480\n",
      "        26.0       0.82      0.74      0.78       180\n",
      "        27.0       0.86      0.42      0.56        60\n",
      "        28.0       0.82      0.84      0.83       150\n",
      "        29.0       0.96      0.54      0.70        90\n",
      "        30.0       0.66      0.63      0.65       150\n",
      "        31.0       0.87      0.96      0.91       270\n",
      "        32.0       0.97      1.00      0.98        60\n",
      "        33.0       0.97      0.96      0.96       210\n",
      "        34.0       1.00      0.97      0.99       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.98      0.94      0.96       120\n",
      "        37.0       1.00      0.60      0.75        60\n",
      "        38.0       0.92      0.99      0.95       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       0.97      0.76      0.85        90\n",
      "        41.0       0.97      0.55      0.70        60\n",
      "        42.0       0.88      0.64      0.74        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.91      0.83      0.86     12630\n",
      "weighted avg       0.89      0.89      0.88     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.8849564528899446\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.86      0.75      0.80       720\n",
      "         2.0       0.76      0.95      0.85       750\n",
      "         3.0       0.94      0.76      0.84       450\n",
      "         4.0       0.90      0.90      0.90       660\n",
      "         5.0       0.76      0.79      0.78       630\n",
      "         6.0       0.82      0.61      0.70       150\n",
      "         7.0       0.85      0.89      0.87       450\n",
      "         8.0       0.79      0.85      0.81       450\n",
      "         9.0       0.88      0.98      0.92       480\n",
      "        10.0       0.89      0.96      0.92       660\n",
      "        11.0       0.87      0.87      0.87       420\n",
      "        12.0       0.97      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.93      0.95       270\n",
      "        15.0       0.96      0.96      0.96       210\n",
      "        16.0       0.90      0.93      0.92       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.83      0.88      0.85       390\n",
      "        19.0       0.98      0.97      0.97        60\n",
      "        20.0       0.95      0.87      0.91        90\n",
      "        21.0       0.98      0.57      0.72        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.87      0.92      0.89       150\n",
      "        24.0       0.98      0.59      0.74        90\n",
      "        25.0       0.80      0.95      0.87       480\n",
      "        26.0       0.84      0.74      0.79       180\n",
      "        27.0       0.86      0.52      0.65        60\n",
      "        28.0       0.81      0.84      0.83       150\n",
      "        29.0       1.00      0.56      0.71        90\n",
      "        30.0       0.67      0.61      0.64       150\n",
      "        31.0       0.87      0.97      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.97      0.97       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.98      0.94      0.96       120\n",
      "        37.0       1.00      0.57      0.72        60\n",
      "        38.0       0.92      0.99      0.95       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       0.99      0.73      0.84        90\n",
      "        41.0       0.97      0.55      0.70        60\n",
      "        42.0       0.91      0.64      0.75        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.91      0.83      0.86     12630\n",
      "weighted avg       0.89      0.88      0.88     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.8905779889152811\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.42      0.59        60\n",
      "         1.0       0.88      0.75      0.81       720\n",
      "         2.0       0.76      0.96      0.85       750\n",
      "         3.0       0.96      0.75      0.84       450\n",
      "         4.0       0.89      0.90      0.90       660\n",
      "         5.0       0.78      0.80      0.79       630\n",
      "         6.0       0.84      0.65      0.73       150\n",
      "         7.0       0.85      0.90      0.88       450\n",
      "         8.0       0.80      0.85      0.82       450\n",
      "         9.0       0.88      0.98      0.93       480\n",
      "        10.0       0.91      0.97      0.94       660\n",
      "        11.0       0.86      0.89      0.87       420\n",
      "        12.0       0.96      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.94      0.96       270\n",
      "        15.0       0.97      0.96      0.96       210\n",
      "        16.0       0.95      0.94      0.94       150\n",
      "        17.0       0.99      0.96      0.97       360\n",
      "        18.0       0.83      0.88      0.86       390\n",
      "        19.0       0.98      0.95      0.97        60\n",
      "        20.0       0.96      0.86      0.91        90\n",
      "        21.0       0.96      0.59      0.73        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.86      0.93      0.89       150\n",
      "        24.0       1.00      0.52      0.69        90\n",
      "        25.0       0.80      0.96      0.87       480\n",
      "        26.0       0.85      0.75      0.80       180\n",
      "        27.0       0.98      0.67      0.79        60\n",
      "        28.0       0.83      0.84      0.84       150\n",
      "        29.0       1.00      0.61      0.76        90\n",
      "        30.0       0.71      0.58      0.64       150\n",
      "        31.0       0.86      0.97      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.97      0.97       210\n",
      "        34.0       1.00      0.97      0.98       120\n",
      "        35.0       0.95      0.98      0.97       390\n",
      "        36.0       0.98      0.94      0.96       120\n",
      "        37.0       1.00      0.60      0.75        60\n",
      "        38.0       0.93      0.99      0.96       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.99      0.74      0.85        90\n",
      "        41.0       1.00      0.52      0.68        60\n",
      "        42.0       0.92      0.67      0.77        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.92      0.83      0.86     12630\n",
      "weighted avg       0.90      0.89      0.89     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.870387965162312\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.10        60\n",
      "         1.0       0.88      0.74      0.80       720\n",
      "         2.0       0.70      0.97      0.81       750\n",
      "         3.0       0.98      0.67      0.80       450\n",
      "         4.0       0.85      0.93      0.89       660\n",
      "         5.0       0.83      0.80      0.81       630\n",
      "         6.0       0.85      0.71      0.77       150\n",
      "         7.0       0.89      0.92      0.90       450\n",
      "         8.0       0.85      0.84      0.84       450\n",
      "         9.0       0.91      0.94      0.92       480\n",
      "        10.0       0.87      0.98      0.92       660\n",
      "        11.0       0.80      0.91      0.85       420\n",
      "        12.0       0.93      1.00      0.96       690\n",
      "        13.0       0.99      1.00      1.00       720\n",
      "        14.0       1.00      0.92      0.96       270\n",
      "        15.0       0.99      0.97      0.98       210\n",
      "        16.0       1.00      0.91      0.95       150\n",
      "        17.0       0.99      1.00      0.99       360\n",
      "        18.0       0.74      0.91      0.82       390\n",
      "        19.0       1.00      0.75      0.86        60\n",
      "        20.0       1.00      0.74      0.85        90\n",
      "        21.0       1.00      0.49      0.66        90\n",
      "        22.0       1.00      0.72      0.84       120\n",
      "        23.0       0.92      0.79      0.85       150\n",
      "        24.0       1.00      0.19      0.32        90\n",
      "        25.0       0.62      1.00      0.76       480\n",
      "        26.0       0.93      0.53      0.68       180\n",
      "        27.0       1.00      0.25      0.40        60\n",
      "        28.0       0.94      0.68      0.79       150\n",
      "        29.0       1.00      0.21      0.35        90\n",
      "        30.0       0.89      0.33      0.48       150\n",
      "        31.0       0.77      0.97      0.86       270\n",
      "        32.0       0.98      0.98      0.98        60\n",
      "        33.0       0.97      0.96      0.96       210\n",
      "        34.0       1.00      0.97      0.98       120\n",
      "        35.0       0.95      0.99      0.97       390\n",
      "        36.0       0.99      0.95      0.97       120\n",
      "        37.0       1.00      0.60      0.75        60\n",
      "        38.0       0.93      1.00      0.96       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.70      0.82        90\n",
      "        41.0       1.00      0.40      0.57        60\n",
      "        42.0       0.95      0.64      0.77        90\n",
      "\n",
      "    accuracy                           0.87     12630\n",
      "   macro avg       0.93      0.77      0.80     12630\n",
      "weighted avg       0.89      0.87      0.86     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8201900237529691\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.06        60\n",
      "         1.0       0.81      0.69      0.74       720\n",
      "         2.0       0.70      0.91      0.79       750\n",
      "         3.0       0.88      0.72      0.79       450\n",
      "         4.0       0.83      0.85      0.84       660\n",
      "         5.0       0.62      0.70      0.66       630\n",
      "         6.0       0.86      0.38      0.53       150\n",
      "         7.0       0.76      0.90      0.83       450\n",
      "         8.0       0.66      0.71      0.68       450\n",
      "         9.0       0.78      0.96      0.86       480\n",
      "        10.0       0.86      0.94      0.90       660\n",
      "        11.0       0.83      0.84      0.83       420\n",
      "        12.0       0.97      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.92      0.90      0.91       270\n",
      "        15.0       0.92      0.95      0.93       210\n",
      "        16.0       0.85      0.67      0.75       150\n",
      "        17.0       0.95      0.92      0.94       360\n",
      "        18.0       0.77      0.88      0.82       390\n",
      "        19.0       0.98      0.83      0.90        60\n",
      "        20.0       0.87      0.83      0.85        90\n",
      "        21.0       0.89      0.19      0.31        90\n",
      "        22.0       0.94      0.73      0.82       120\n",
      "        23.0       0.75      0.81      0.78       150\n",
      "        24.0       1.00      0.20      0.33        90\n",
      "        25.0       0.75      0.93      0.83       480\n",
      "        26.0       0.72      0.71      0.71       180\n",
      "        27.0       1.00      0.18      0.31        60\n",
      "        28.0       0.80      0.59      0.68       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.52      0.43      0.47       150\n",
      "        31.0       0.72      0.96      0.82       270\n",
      "        32.0       0.98      0.90      0.94        60\n",
      "        33.0       0.93      0.93      0.93       210\n",
      "        34.0       0.94      0.91      0.92       120\n",
      "        35.0       0.85      0.97      0.91       390\n",
      "        36.0       0.98      0.82      0.89       120\n",
      "        37.0       0.94      0.27      0.42        60\n",
      "        38.0       0.91      0.97      0.94       690\n",
      "        39.0       1.00      0.69      0.82        90\n",
      "        40.0       1.00      0.20      0.33        90\n",
      "        41.0       1.00      0.27      0.42        60\n",
      "        42.0       0.96      0.54      0.70        90\n",
      "\n",
      "    accuracy                           0.82     12630\n",
      "   macro avg       0.85      0.69      0.72     12630\n",
      "weighted avg       0.83      0.82      0.81     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8229612034837688\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.81      0.70      0.75       720\n",
      "         2.0       0.69      0.92      0.79       750\n",
      "         3.0       0.89      0.72      0.80       450\n",
      "         4.0       0.82      0.86      0.84       660\n",
      "         5.0       0.65      0.70      0.67       630\n",
      "         6.0       0.89      0.39      0.55       150\n",
      "         7.0       0.76      0.90      0.82       450\n",
      "         8.0       0.70      0.70      0.70       450\n",
      "         9.0       0.76      0.97      0.85       480\n",
      "        10.0       0.85      0.95      0.90       660\n",
      "        11.0       0.82      0.87      0.85       420\n",
      "        12.0       0.95      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.92      0.90      0.91       270\n",
      "        15.0       0.92      0.95      0.93       210\n",
      "        16.0       0.91      0.66      0.76       150\n",
      "        17.0       0.96      0.93      0.94       360\n",
      "        18.0       0.74      0.89      0.81       390\n",
      "        19.0       1.00      0.77      0.87        60\n",
      "        20.0       0.94      0.80      0.86        90\n",
      "        21.0       0.91      0.22      0.36        90\n",
      "        22.0       0.97      0.73      0.83       120\n",
      "        23.0       0.73      0.77      0.75       150\n",
      "        24.0       1.00      0.18      0.30        90\n",
      "        25.0       0.74      0.94      0.82       480\n",
      "        26.0       0.75      0.67      0.71       180\n",
      "        27.0       1.00      0.17      0.29        60\n",
      "        28.0       0.84      0.59      0.69       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.54      0.41      0.47       150\n",
      "        31.0       0.70      0.97      0.81       270\n",
      "        32.0       0.98      0.90      0.94        60\n",
      "        33.0       0.92      0.93      0.93       210\n",
      "        34.0       0.95      0.92      0.93       120\n",
      "        35.0       0.86      0.98      0.92       390\n",
      "        36.0       0.99      0.84      0.91       120\n",
      "        37.0       1.00      0.30      0.46        60\n",
      "        38.0       0.91      0.98      0.94       690\n",
      "        39.0       1.00      0.78      0.88        90\n",
      "        40.0       1.00      0.24      0.39        90\n",
      "        41.0       1.00      0.08      0.15        60\n",
      "        42.0       0.98      0.53      0.69        90\n",
      "\n",
      "    accuracy                           0.82     12630\n",
      "   macro avg       0.83      0.69      0.71     12630\n",
      "weighted avg       0.83      0.82      0.81     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8263657957244656\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.83      0.70      0.76       720\n",
      "         2.0       0.69      0.93      0.79       750\n",
      "         3.0       0.92      0.72      0.81       450\n",
      "         4.0       0.81      0.87      0.84       660\n",
      "         5.0       0.68      0.70      0.69       630\n",
      "         6.0       0.88      0.44      0.59       150\n",
      "         7.0       0.76      0.90      0.83       450\n",
      "         8.0       0.74      0.71      0.72       450\n",
      "         9.0       0.74      0.97      0.84       480\n",
      "        10.0       0.86      0.95      0.90       660\n",
      "        11.0       0.79      0.89      0.84       420\n",
      "        12.0       0.95      0.99      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.95      0.92      0.94       270\n",
      "        15.0       0.94      0.94      0.94       210\n",
      "        16.0       0.98      0.63      0.76       150\n",
      "        17.0       0.97      0.95      0.96       360\n",
      "        18.0       0.74      0.92      0.82       390\n",
      "        19.0       1.00      0.43      0.60        60\n",
      "        20.0       0.99      0.78      0.87        90\n",
      "        21.0       0.95      0.20      0.33        90\n",
      "        22.0       0.99      0.73      0.84       120\n",
      "        23.0       0.73      0.76      0.74       150\n",
      "        24.0       1.00      0.18      0.30        90\n",
      "        25.0       0.71      0.96      0.82       480\n",
      "        26.0       0.79      0.59      0.67       180\n",
      "        27.0       1.00      0.15      0.26        60\n",
      "        28.0       0.89      0.58      0.70       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.48      0.37      0.42       150\n",
      "        31.0       0.69      0.97      0.80       270\n",
      "        32.0       1.00      0.88      0.94        60\n",
      "        33.0       0.95      0.94      0.94       210\n",
      "        34.0       0.96      0.91      0.94       120\n",
      "        35.0       0.87      0.98      0.92       390\n",
      "        36.0       0.99      0.88      0.93       120\n",
      "        37.0       1.00      0.30      0.46        60\n",
      "        38.0       0.90      0.98      0.94       690\n",
      "        39.0       1.00      0.87      0.93        90\n",
      "        40.0       1.00      0.28      0.43        90\n",
      "        41.0       1.00      0.02      0.03        60\n",
      "        42.0       0.98      0.54      0.70        90\n",
      "\n",
      "    accuracy                           0.83     12630\n",
      "   macro avg       0.84      0.68      0.71     12630\n",
      "weighted avg       0.83      0.83      0.81     12630\n",
      "\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7853523357086303\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.81      0.69      0.75       720\n",
      "         2.0       0.61      0.94      0.74       750\n",
      "         3.0       0.97      0.63      0.76       450\n",
      "         4.0       0.77      0.92      0.84       660\n",
      "         5.0       0.81      0.65      0.72       630\n",
      "         6.0       0.87      0.45      0.60       150\n",
      "         7.0       0.84      0.89      0.87       450\n",
      "         8.0       0.86      0.71      0.78       450\n",
      "         9.0       0.80      0.87      0.83       480\n",
      "        10.0       0.74      0.97      0.84       660\n",
      "        11.0       0.72      0.88      0.79       420\n",
      "        12.0       0.89      1.00      0.94       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       1.00      0.91      0.95       270\n",
      "        15.0       0.98      0.88      0.93       210\n",
      "        16.0       1.00      0.49      0.65       150\n",
      "        17.0       0.98      0.99      0.99       360\n",
      "        18.0       0.60      0.92      0.73       390\n",
      "        19.0       0.00      0.00      0.00        60\n",
      "        20.0       1.00      0.46      0.63        90\n",
      "        21.0       1.00      0.04      0.09        90\n",
      "        22.0       1.00      0.62      0.76       120\n",
      "        23.0       1.00      0.37      0.54       150\n",
      "        24.0       0.00      0.00      0.00        90\n",
      "        25.0       0.47      0.99      0.64       480\n",
      "        26.0       1.00      0.03      0.05       180\n",
      "        27.0       0.00      0.00      0.00        60\n",
      "        28.0       1.00      0.11      0.19       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       1.00      0.13      0.24       150\n",
      "        31.0       0.63      0.94      0.76       270\n",
      "        32.0       1.00      0.67      0.80        60\n",
      "        33.0       0.97      0.91      0.94       210\n",
      "        34.0       1.00      0.78      0.88       120\n",
      "        35.0       0.86      0.98      0.92       390\n",
      "        36.0       1.00      0.78      0.88       120\n",
      "        37.0       1.00      0.23      0.38        60\n",
      "        38.0       0.83      1.00      0.91       690\n",
      "        39.0       1.00      0.86      0.92        90\n",
      "        40.0       1.00      0.21      0.35        90\n",
      "        41.0       0.00      0.00      0.00        60\n",
      "        42.0       0.97      0.36      0.52        90\n",
      "\n",
      "    accuracy                           0.79     12630\n",
      "   macro avg       0.77      0.59      0.61     12630\n",
      "weighted avg       0.80      0.79      0.76     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9104513064133016\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.73      0.85        60\n",
      "         1.0       0.89      0.79      0.84       720\n",
      "         2.0       0.80      0.95      0.87       750\n",
      "         3.0       0.95      0.79      0.86       450\n",
      "         4.0       0.93      0.93      0.93       660\n",
      "         5.0       0.82      0.85      0.84       630\n",
      "         6.0       0.82      0.71      0.76       150\n",
      "         7.0       0.89      0.91      0.90       450\n",
      "         8.0       0.85      0.88      0.87       450\n",
      "         9.0       0.90      0.98      0.94       480\n",
      "        10.0       0.91      0.98      0.94       660\n",
      "        11.0       0.88      0.89      0.89       420\n",
      "        12.0       0.98      0.99      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.95      0.96       270\n",
      "        15.0       0.97      0.97      0.97       210\n",
      "        16.0       0.95      0.96      0.96       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.89      0.88      0.88       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.91      0.87      0.89        90\n",
      "        21.0       0.97      0.67      0.79        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.87      0.95      0.90       150\n",
      "        24.0       1.00      0.78      0.88        90\n",
      "        25.0       0.84      0.95      0.89       480\n",
      "        26.0       0.83      0.77      0.80       180\n",
      "        27.0       0.97      0.62      0.76        60\n",
      "        28.0       0.83      0.90      0.87       150\n",
      "        29.0       0.96      0.88      0.92        90\n",
      "        30.0       0.85      0.67      0.75       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.98       210\n",
      "        34.0       0.99      0.99      0.99       120\n",
      "        35.0       0.95      0.98      0.96       390\n",
      "        36.0       0.99      0.96      0.97       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.97      0.80      0.88        90\n",
      "        41.0       0.97      0.65      0.78        60\n",
      "        42.0       0.92      0.62      0.74        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.87      0.90     12630\n",
      "weighted avg       0.91      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.913064133016627\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80        60\n",
      "         1.0       0.89      0.80      0.84       720\n",
      "         2.0       0.80      0.96      0.87       750\n",
      "         3.0       0.95      0.78      0.85       450\n",
      "         4.0       0.92      0.93      0.93       660\n",
      "         5.0       0.82      0.86      0.84       630\n",
      "         6.0       0.84      0.72      0.78       150\n",
      "         7.0       0.91      0.92      0.91       450\n",
      "         8.0       0.86      0.88      0.87       450\n",
      "         9.0       0.90      0.98      0.94       480\n",
      "        10.0       0.94      0.98      0.96       660\n",
      "        11.0       0.88      0.91      0.89       420\n",
      "        12.0       0.98      0.99      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.95      0.97       270\n",
      "        15.0       0.98      0.97      0.98       210\n",
      "        16.0       0.96      0.97      0.96       150\n",
      "        17.0       0.98      0.96      0.97       360\n",
      "        18.0       0.90      0.88      0.89       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.94      0.88      0.91        90\n",
      "        21.0       0.98      0.64      0.78        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.94      0.91       150\n",
      "        24.0       0.99      0.83      0.90        90\n",
      "        25.0       0.84      0.95      0.89       480\n",
      "        26.0       0.85      0.77      0.81       180\n",
      "        27.0       0.93      0.68      0.79        60\n",
      "        28.0       0.87      0.88      0.87       150\n",
      "        29.0       0.94      0.83      0.88        90\n",
      "        30.0       0.81      0.67      0.73       150\n",
      "        31.0       0.88      0.98      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.97       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.77      0.87        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.99      0.80      0.88        90\n",
      "        41.0       0.95      0.58      0.72        60\n",
      "        42.0       0.91      0.67      0.77        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.87      0.90     12630\n",
      "weighted avg       0.92      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.9184481393507522\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.57      0.72        60\n",
      "         1.0       0.90      0.81      0.85       720\n",
      "         2.0       0.80      0.96      0.87       750\n",
      "         3.0       0.96      0.78      0.86       450\n",
      "         4.0       0.92      0.94      0.93       660\n",
      "         5.0       0.84      0.86      0.85       630\n",
      "         6.0       0.86      0.75      0.80       150\n",
      "         7.0       0.91      0.91      0.91       450\n",
      "         8.0       0.84      0.88      0.86       450\n",
      "         9.0       0.90      0.99      0.94       480\n",
      "        10.0       0.95      0.98      0.97       660\n",
      "        11.0       0.89      0.92      0.91       420\n",
      "        12.0       0.99      0.99      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.95      0.97       270\n",
      "        15.0       0.99      0.98      0.98       210\n",
      "        16.0       0.98      0.97      0.97       150\n",
      "        17.0       0.99      0.96      0.97       360\n",
      "        18.0       0.92      0.90      0.91       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       1.00      0.61      0.76        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.97      0.92       150\n",
      "        24.0       1.00      0.80      0.89        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.84      0.77      0.81       180\n",
      "        27.0       1.00      0.75      0.86        60\n",
      "        28.0       0.86      0.88      0.87       150\n",
      "        29.0       0.98      0.93      0.95        90\n",
      "        30.0       0.93      0.66      0.77       150\n",
      "        31.0       0.90      0.97      0.94       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.99      0.98       210\n",
      "        34.0       1.00      0.99      1.00       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.97      0.82      0.89        90\n",
      "        41.0       1.00      0.60      0.75        60\n",
      "        42.0       0.93      0.70      0.80        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.88      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.9038796516231196\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.27      0.42        60\n",
      "         1.0       0.90      0.78      0.84       720\n",
      "         2.0       0.75      0.97      0.84       750\n",
      "         3.0       0.99      0.73      0.84       450\n",
      "         4.0       0.89      0.94      0.91       660\n",
      "         5.0       0.84      0.84      0.84       630\n",
      "         6.0       0.84      0.77      0.80       150\n",
      "         7.0       0.90      0.93      0.91       450\n",
      "         8.0       0.88      0.86      0.87       450\n",
      "         9.0       0.93      0.96      0.94       480\n",
      "        10.0       0.92      0.99      0.95       660\n",
      "        11.0       0.83      0.94      0.88       420\n",
      "        12.0       0.96      0.99      0.98       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       1.00      0.95      0.97       270\n",
      "        15.0       0.99      0.99      0.99       210\n",
      "        16.0       1.00      0.95      0.98       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.89      0.91      0.90       390\n",
      "        19.0       1.00      0.92      0.96        60\n",
      "        20.0       1.00      0.81      0.90        90\n",
      "        21.0       0.96      0.59      0.73        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.88      0.86      0.87       150\n",
      "        24.0       1.00      0.68      0.81        90\n",
      "        25.0       0.72      1.00      0.84       480\n",
      "        26.0       0.90      0.72      0.80       180\n",
      "        27.0       1.00      0.57      0.72        60\n",
      "        28.0       0.90      0.81      0.85       150\n",
      "        29.0       1.00      0.83      0.91        90\n",
      "        30.0       1.00      0.38      0.55       150\n",
      "        31.0       0.84      0.98      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.99      0.98      0.98       210\n",
      "        34.0       1.00      0.97      0.98       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.99      0.96      0.97       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.94      1.00      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.80      0.89        90\n",
      "        41.0       1.00      0.53      0.70        60\n",
      "        42.0       0.95      0.69      0.80        90\n",
      "\n",
      "    accuracy                           0.90     12630\n",
      "   macro avg       0.94      0.84      0.88     12630\n",
      "weighted avg       0.91      0.90      0.90     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8854315122723674\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.57      0.72        60\n",
      "         1.0       0.87      0.75      0.80       720\n",
      "         2.0       0.78      0.95      0.85       750\n",
      "         3.0       0.94      0.76      0.84       450\n",
      "         4.0       0.90      0.91      0.90       660\n",
      "         5.0       0.77      0.80      0.78       630\n",
      "         6.0       0.85      0.64      0.73       150\n",
      "         7.0       0.85      0.89      0.87       450\n",
      "         8.0       0.78      0.85      0.82       450\n",
      "         9.0       0.89      0.97      0.93       480\n",
      "        10.0       0.88      0.96      0.92       660\n",
      "        11.0       0.88      0.86      0.87       420\n",
      "        12.0       0.97      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.92      0.95       270\n",
      "        15.0       0.98      0.96      0.97       210\n",
      "        16.0       0.84      0.94      0.89       150\n",
      "        17.0       0.97      0.93      0.95       360\n",
      "        18.0       0.83      0.88      0.86       390\n",
      "        19.0       0.98      0.97      0.97        60\n",
      "        20.0       0.93      0.87      0.90        90\n",
      "        21.0       0.98      0.56      0.71        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.86      0.92      0.89       150\n",
      "        24.0       0.98      0.61      0.75        90\n",
      "        25.0       0.82      0.95      0.88       480\n",
      "        26.0       0.83      0.74      0.79       180\n",
      "        27.0       0.76      0.32      0.45        60\n",
      "        28.0       0.81      0.85      0.83       150\n",
      "        29.0       1.00      0.59      0.74        90\n",
      "        30.0       0.66      0.63      0.64       150\n",
      "        31.0       0.87      0.96      0.91       270\n",
      "        32.0       0.97      1.00      0.98        60\n",
      "        33.0       0.96      0.96      0.96       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.98      0.93      0.95       120\n",
      "        37.0       1.00      0.65      0.79        60\n",
      "        38.0       0.92      0.99      0.95       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       0.99      0.74      0.85        90\n",
      "        41.0       0.97      0.62      0.76        60\n",
      "        42.0       0.89      0.64      0.75        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.91      0.83      0.86     12630\n",
      "weighted avg       0.89      0.89      0.88     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.8842438638163104\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.52      0.68        60\n",
      "         1.0       0.87      0.74      0.80       720\n",
      "         2.0       0.77      0.95      0.85       750\n",
      "         3.0       0.94      0.76      0.84       450\n",
      "         4.0       0.91      0.90      0.90       660\n",
      "         5.0       0.77      0.80      0.78       630\n",
      "         6.0       0.82      0.63      0.71       150\n",
      "         7.0       0.85      0.89      0.87       450\n",
      "         8.0       0.78      0.85      0.81       450\n",
      "         9.0       0.88      0.97      0.93       480\n",
      "        10.0       0.88      0.96      0.92       660\n",
      "        11.0       0.87      0.87      0.87       420\n",
      "        12.0       0.97      0.99      0.98       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.98      0.93      0.95       270\n",
      "        15.0       0.96      0.95      0.95       210\n",
      "        16.0       0.87      0.93      0.90       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.83      0.87      0.85       390\n",
      "        19.0       0.98      0.97      0.97        60\n",
      "        20.0       0.93      0.87      0.90        90\n",
      "        21.0       0.98      0.52      0.68        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.86      0.92      0.89       150\n",
      "        24.0       1.00      0.59      0.74        90\n",
      "        25.0       0.81      0.95      0.88       480\n",
      "        26.0       0.81      0.76      0.78       180\n",
      "        27.0       0.90      0.47      0.62        60\n",
      "        28.0       0.83      0.85      0.84       150\n",
      "        29.0       1.00      0.58      0.73        90\n",
      "        30.0       0.69      0.63      0.66       150\n",
      "        31.0       0.86      0.97      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.94      0.96       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.93      0.98      0.95       390\n",
      "        36.0       0.99      0.93      0.96       120\n",
      "        37.0       1.00      0.62      0.76        60\n",
      "        38.0       0.92      0.99      0.95       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       0.97      0.70      0.81        90\n",
      "        41.0       0.97      0.57      0.72        60\n",
      "        42.0       0.90      0.63      0.75        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.91      0.83      0.86     12630\n",
      "weighted avg       0.89      0.88      0.88     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.8892319873317498\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.38      0.55        60\n",
      "         1.0       0.88      0.75      0.81       720\n",
      "         2.0       0.76      0.95      0.85       750\n",
      "         3.0       0.94      0.75      0.84       450\n",
      "         4.0       0.90      0.90      0.90       660\n",
      "         5.0       0.78      0.80      0.79       630\n",
      "         6.0       0.84      0.65      0.73       150\n",
      "         7.0       0.85      0.90      0.88       450\n",
      "         8.0       0.80      0.86      0.83       450\n",
      "         9.0       0.87      0.98      0.92       480\n",
      "        10.0       0.90      0.97      0.94       660\n",
      "        11.0       0.88      0.88      0.88       420\n",
      "        12.0       0.97      0.99      0.98       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.93      0.96       270\n",
      "        15.0       0.98      0.96      0.97       210\n",
      "        16.0       0.95      0.91      0.93       150\n",
      "        17.0       0.98      0.96      0.97       360\n",
      "        18.0       0.83      0.88      0.85       390\n",
      "        19.0       1.00      0.95      0.97        60\n",
      "        20.0       0.95      0.84      0.89        90\n",
      "        21.0       0.98      0.59      0.74        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.88      0.90      0.89       150\n",
      "        24.0       1.00      0.57      0.72        90\n",
      "        25.0       0.80      0.96      0.87       480\n",
      "        26.0       0.84      0.75      0.79       180\n",
      "        27.0       0.92      0.60      0.73        60\n",
      "        28.0       0.83      0.83      0.83       150\n",
      "        29.0       1.00      0.61      0.76        90\n",
      "        30.0       0.70      0.62      0.66       150\n",
      "        31.0       0.86      0.97      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.97       210\n",
      "        34.0       1.00      0.97      0.99       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.99      0.94      0.97       120\n",
      "        37.0       1.00      0.58      0.74        60\n",
      "        38.0       0.92      0.99      0.96       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.73      0.85        90\n",
      "        41.0       1.00      0.52      0.68        60\n",
      "        42.0       0.92      0.64      0.76        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.92      0.83      0.86     12630\n",
      "weighted avg       0.89      0.89      0.89     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.869200316706255\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.06        60\n",
      "         1.0       0.89      0.74      0.80       720\n",
      "         2.0       0.72      0.96      0.82       750\n",
      "         3.0       0.98      0.70      0.81       450\n",
      "         4.0       0.85      0.93      0.89       660\n",
      "         5.0       0.82      0.80      0.81       630\n",
      "         6.0       0.83      0.69      0.76       150\n",
      "         7.0       0.89      0.91      0.90       450\n",
      "         8.0       0.84      0.84      0.84       450\n",
      "         9.0       0.91      0.94      0.93       480\n",
      "        10.0       0.87      0.98      0.92       660\n",
      "        11.0       0.82      0.90      0.86       420\n",
      "        12.0       0.92      1.00      0.96       690\n",
      "        13.0       0.99      1.00      1.00       720\n",
      "        14.0       0.99      0.93      0.96       270\n",
      "        15.0       0.98      0.97      0.97       210\n",
      "        16.0       1.00      0.92      0.96       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.76      0.92      0.83       390\n",
      "        19.0       1.00      0.68      0.81        60\n",
      "        20.0       1.00      0.77      0.87        90\n",
      "        21.0       1.00      0.53      0.70        90\n",
      "        22.0       1.00      0.71      0.83       120\n",
      "        23.0       0.92      0.80      0.86       150\n",
      "        24.0       1.00      0.22      0.36        90\n",
      "        25.0       0.60      0.99      0.75       480\n",
      "        26.0       0.91      0.53      0.67       180\n",
      "        27.0       1.00      0.23      0.38        60\n",
      "        28.0       0.93      0.67      0.78       150\n",
      "        29.0       1.00      0.19      0.32        90\n",
      "        30.0       0.78      0.28      0.41       150\n",
      "        31.0       0.77      0.97      0.85       270\n",
      "        32.0       0.98      0.93      0.96        60\n",
      "        33.0       0.98      0.96      0.97       210\n",
      "        34.0       1.00      0.97      0.98       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.99      0.93      0.96       120\n",
      "        37.0       1.00      0.52      0.68        60\n",
      "        38.0       0.93      1.00      0.96       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.64      0.78        90\n",
      "        41.0       0.96      0.43      0.60        60\n",
      "        42.0       0.95      0.62      0.75        90\n",
      "\n",
      "    accuracy                           0.87     12630\n",
      "   macro avg       0.92      0.76      0.80     12630\n",
      "weighted avg       0.89      0.87      0.86     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8204275534441805\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.03      0.06        60\n",
      "         1.0       0.80      0.70      0.74       720\n",
      "         2.0       0.71      0.91      0.80       750\n",
      "         3.0       0.88      0.72      0.79       450\n",
      "         4.0       0.82      0.85      0.83       660\n",
      "         5.0       0.62      0.71      0.66       630\n",
      "         6.0       0.92      0.39      0.54       150\n",
      "         7.0       0.77      0.89      0.82       450\n",
      "         8.0       0.67      0.71      0.69       450\n",
      "         9.0       0.77      0.96      0.86       480\n",
      "        10.0       0.86      0.93      0.89       660\n",
      "        11.0       0.84      0.84      0.84       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.92      0.90      0.91       270\n",
      "        15.0       0.91      0.93      0.92       210\n",
      "        16.0       0.83      0.67      0.74       150\n",
      "        17.0       0.96      0.92      0.94       360\n",
      "        18.0       0.76      0.87      0.81       390\n",
      "        19.0       0.98      0.83      0.90        60\n",
      "        20.0       0.83      0.82      0.83        90\n",
      "        21.0       0.90      0.20      0.33        90\n",
      "        22.0       0.96      0.73      0.83       120\n",
      "        23.0       0.74      0.81      0.77       150\n",
      "        24.0       1.00      0.18      0.30        90\n",
      "        25.0       0.75      0.93      0.83       480\n",
      "        26.0       0.69      0.69      0.69       180\n",
      "        27.0       1.00      0.18      0.31        60\n",
      "        28.0       0.83      0.63      0.71       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.53      0.44      0.48       150\n",
      "        31.0       0.71      0.96      0.82       270\n",
      "        32.0       1.00      0.82      0.90        60\n",
      "        33.0       0.91      0.93      0.92       210\n",
      "        34.0       0.96      0.90      0.93       120\n",
      "        35.0       0.85      0.97      0.90       390\n",
      "        36.0       0.97      0.86      0.91       120\n",
      "        37.0       0.94      0.28      0.44        60\n",
      "        38.0       0.90      0.97      0.93       690\n",
      "        39.0       1.00      0.74      0.85        90\n",
      "        40.0       1.00      0.21      0.35        90\n",
      "        41.0       1.00      0.25      0.40        60\n",
      "        42.0       0.96      0.53      0.69        90\n",
      "\n",
      "    accuracy                           0.82     12630\n",
      "   macro avg       0.85      0.69      0.72     12630\n",
      "weighted avg       0.83      0.82      0.81     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8254156769596199\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.81      0.70      0.75       720\n",
      "         2.0       0.71      0.91      0.80       750\n",
      "         3.0       0.88      0.73      0.80       450\n",
      "         4.0       0.83      0.86      0.84       660\n",
      "         5.0       0.66      0.71      0.68       630\n",
      "         6.0       0.85      0.42      0.56       150\n",
      "         7.0       0.77      0.89      0.82       450\n",
      "         8.0       0.68      0.71      0.70       450\n",
      "         9.0       0.76      0.96      0.85       480\n",
      "        10.0       0.85      0.95      0.90       660\n",
      "        11.0       0.82      0.87      0.85       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.93      0.91      0.92       270\n",
      "        15.0       0.92      0.94      0.93       210\n",
      "        16.0       0.91      0.67      0.77       150\n",
      "        17.0       0.96      0.93      0.94       360\n",
      "        18.0       0.75      0.89      0.82       390\n",
      "        19.0       1.00      0.77      0.87        60\n",
      "        20.0       0.91      0.80      0.85        90\n",
      "        21.0       0.95      0.21      0.35        90\n",
      "        22.0       0.97      0.72      0.83       120\n",
      "        23.0       0.73      0.79      0.76       150\n",
      "        24.0       1.00      0.18      0.30        90\n",
      "        25.0       0.75      0.94      0.83       480\n",
      "        26.0       0.76      0.66      0.71       180\n",
      "        27.0       1.00      0.17      0.29        60\n",
      "        28.0       0.83      0.62      0.71       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.54      0.44      0.49       150\n",
      "        31.0       0.70      0.97      0.81       270\n",
      "        32.0       0.98      0.88      0.93        60\n",
      "        33.0       0.92      0.93      0.92       210\n",
      "        34.0       0.96      0.90      0.93       120\n",
      "        35.0       0.85      0.98      0.91       390\n",
      "        36.0       1.00      0.88      0.93       120\n",
      "        37.0       1.00      0.33      0.50        60\n",
      "        38.0       0.90      0.98      0.94       690\n",
      "        39.0       1.00      0.76      0.86        90\n",
      "        40.0       1.00      0.23      0.38        90\n",
      "        41.0       1.00      0.13      0.24        60\n",
      "        42.0       0.98      0.52      0.68        90\n",
      "\n",
      "    accuracy                           0.83     12630\n",
      "   macro avg       0.83      0.69      0.72     12630\n",
      "weighted avg       0.83      0.83      0.81     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8234362628661916\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.81      0.70      0.75       720\n",
      "         2.0       0.69      0.93      0.79       750\n",
      "         3.0       0.92      0.72      0.81       450\n",
      "         4.0       0.81      0.87      0.84       660\n",
      "         5.0       0.68      0.70      0.69       630\n",
      "         6.0       0.88      0.43      0.58       150\n",
      "         7.0       0.77      0.91      0.83       450\n",
      "         8.0       0.73      0.69      0.71       450\n",
      "         9.0       0.76      0.97      0.85       480\n",
      "        10.0       0.86      0.95      0.90       660\n",
      "        11.0       0.79      0.88      0.83       420\n",
      "        12.0       0.95      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.96      0.92      0.94       270\n",
      "        15.0       0.93      0.93      0.93       210\n",
      "        16.0       0.97      0.61      0.75       150\n",
      "        17.0       0.97      0.96      0.97       360\n",
      "        18.0       0.73      0.91      0.81       390\n",
      "        19.0       1.00      0.50      0.67        60\n",
      "        20.0       1.00      0.74      0.85        90\n",
      "        21.0       0.95      0.20      0.33        90\n",
      "        22.0       0.99      0.72      0.83       120\n",
      "        23.0       0.74      0.77      0.76       150\n",
      "        24.0       1.00      0.13      0.24        90\n",
      "        25.0       0.69      0.96      0.80       480\n",
      "        26.0       0.80      0.59      0.68       180\n",
      "        27.0       1.00      0.13      0.24        60\n",
      "        28.0       0.87      0.57      0.69       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.49      0.33      0.40       150\n",
      "        31.0       0.68      0.97      0.80       270\n",
      "        32.0       1.00      0.92      0.96        60\n",
      "        33.0       0.93      0.93      0.93       210\n",
      "        34.0       0.98      0.90      0.94       120\n",
      "        35.0       0.87      0.98      0.92       390\n",
      "        36.0       1.00      0.86      0.92       120\n",
      "        37.0       1.00      0.30      0.46        60\n",
      "        38.0       0.90      0.98      0.94       690\n",
      "        39.0       1.00      0.78      0.88        90\n",
      "        40.0       1.00      0.24      0.39        90\n",
      "        41.0       1.00      0.02      0.03        60\n",
      "        42.0       0.98      0.53      0.69        90\n",
      "\n",
      "    accuracy                           0.82     12630\n",
      "   macro avg       0.84      0.68      0.70     12630\n",
      "weighted avg       0.83      0.82      0.81     12630\n",
      "\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7836104513064133\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.80      0.69      0.74       720\n",
      "         2.0       0.62      0.95      0.75       750\n",
      "         3.0       0.98      0.62      0.76       450\n",
      "         4.0       0.77      0.92      0.83       660\n",
      "         5.0       0.79      0.66      0.72       630\n",
      "         6.0       0.96      0.44      0.60       150\n",
      "         7.0       0.84      0.89      0.86       450\n",
      "         8.0       0.87      0.69      0.77       450\n",
      "         9.0       0.78      0.85      0.82       480\n",
      "        10.0       0.73      0.97      0.83       660\n",
      "        11.0       0.71      0.88      0.79       420\n",
      "        12.0       0.88      1.00      0.93       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       1.00      0.90      0.94       270\n",
      "        15.0       0.98      0.89      0.93       210\n",
      "        16.0       1.00      0.42      0.59       150\n",
      "        17.0       0.98      0.99      0.99       360\n",
      "        18.0       0.62      0.91      0.73       390\n",
      "        19.0       0.00      0.00      0.00        60\n",
      "        20.0       1.00      0.52      0.69        90\n",
      "        21.0       1.00      0.03      0.06        90\n",
      "        22.0       1.00      0.63      0.78       120\n",
      "        23.0       1.00      0.35      0.51       150\n",
      "        24.0       0.00      0.00      0.00        90\n",
      "        25.0       0.46      0.99      0.63       480\n",
      "        26.0       0.94      0.08      0.15       180\n",
      "        27.0       0.00      0.00      0.00        60\n",
      "        28.0       1.00      0.09      0.16       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       1.00      0.13      0.24       150\n",
      "        31.0       0.62      0.94      0.75       270\n",
      "        32.0       1.00      0.62      0.76        60\n",
      "        33.0       0.98      0.90      0.94       210\n",
      "        34.0       1.00      0.78      0.88       120\n",
      "        35.0       0.86      0.98      0.92       390\n",
      "        36.0       1.00      0.82      0.90       120\n",
      "        37.0       1.00      0.27      0.42        60\n",
      "        38.0       0.84      1.00      0.91       690\n",
      "        39.0       1.00      0.87      0.93        90\n",
      "        40.0       1.00      0.28      0.43        90\n",
      "        41.0       0.00      0.00      0.00        60\n",
      "        42.0       0.96      0.30      0.46        90\n",
      "\n",
      "    accuracy                           0.78     12630\n",
      "   macro avg       0.77      0.59      0.61     12630\n",
      "weighted avg       0.80      0.78      0.76     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9034837688044339\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.68      0.81        60\n",
      "         1.0       0.87      0.79      0.83       720\n",
      "         2.0       0.80      0.94      0.86       750\n",
      "         3.0       0.94      0.78      0.85       450\n",
      "         4.0       0.91      0.93      0.92       660\n",
      "         5.0       0.79      0.82      0.80       630\n",
      "         6.0       0.84      0.75      0.79       150\n",
      "         7.0       0.87      0.90      0.89       450\n",
      "         8.0       0.84      0.86      0.85       450\n",
      "         9.0       0.89      0.97      0.93       480\n",
      "        10.0       0.92      0.97      0.95       660\n",
      "        11.0       0.89      0.90      0.89       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.94      0.96       270\n",
      "        15.0       0.97      0.97      0.97       210\n",
      "        16.0       0.95      0.95      0.95       150\n",
      "        17.0       0.98      0.94      0.96       360\n",
      "        18.0       0.89      0.88      0.88       390\n",
      "        19.0       0.98      1.00      0.99        60\n",
      "        20.0       0.92      0.88      0.90        90\n",
      "        21.0       0.98      0.64      0.78        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.89      0.95      0.92       150\n",
      "        24.0       0.99      0.79      0.88        90\n",
      "        25.0       0.84      0.96      0.89       480\n",
      "        26.0       0.83      0.76      0.79       180\n",
      "        27.0       0.95      0.67      0.78        60\n",
      "        28.0       0.81      0.87      0.84       150\n",
      "        29.0       0.96      0.81      0.88        90\n",
      "        30.0       0.76      0.63      0.69       150\n",
      "        31.0       0.88      0.97      0.92       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.96      0.96       210\n",
      "        34.0       0.98      0.97      0.98       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.98      0.95      0.97       120\n",
      "        37.0       0.98      0.75      0.85        60\n",
      "        38.0       0.94      0.99      0.96       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       0.97      0.77      0.86        90\n",
      "        41.0       0.93      0.63      0.75        60\n",
      "        42.0       0.90      0.61      0.73        90\n",
      "\n",
      "    accuracy                           0.90     12630\n",
      "   macro avg       0.92      0.87      0.89     12630\n",
      "weighted avg       0.91      0.90      0.90     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.9072842438638163\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.63      0.78        60\n",
      "         1.0       0.89      0.78      0.83       720\n",
      "         2.0       0.79      0.96      0.87       750\n",
      "         3.0       0.95      0.78      0.86       450\n",
      "         4.0       0.92      0.93      0.93       660\n",
      "         5.0       0.77      0.83      0.80       630\n",
      "         6.0       0.84      0.74      0.79       150\n",
      "         7.0       0.88      0.90      0.89       450\n",
      "         8.0       0.84      0.86      0.85       450\n",
      "         9.0       0.90      0.98      0.94       480\n",
      "        10.0       0.94      0.97      0.96       660\n",
      "        11.0       0.88      0.90      0.89       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.98      0.94      0.96       270\n",
      "        15.0       0.98      0.96      0.97       210\n",
      "        16.0       0.97      0.96      0.96       150\n",
      "        17.0       0.98      0.96      0.97       360\n",
      "        18.0       0.91      0.88      0.90       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.95      0.88      0.91        90\n",
      "        21.0       0.98      0.69      0.81        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.88      0.94      0.91       150\n",
      "        24.0       0.99      0.80      0.88        90\n",
      "        25.0       0.84      0.95      0.89       480\n",
      "        26.0       0.80      0.78      0.79       180\n",
      "        27.0       0.88      0.48      0.62        60\n",
      "        28.0       0.81      0.89      0.85       150\n",
      "        29.0       0.96      0.84      0.90        90\n",
      "        30.0       0.79      0.69      0.73       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.97       210\n",
      "        34.0       1.00      0.99      1.00       120\n",
      "        35.0       0.95      0.98      0.97       390\n",
      "        36.0       0.99      0.96      0.97       120\n",
      "        37.0       1.00      0.73      0.85        60\n",
      "        38.0       0.94      0.99      0.96       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.97      0.78      0.86        90\n",
      "        41.0       0.97      0.60      0.74        60\n",
      "        42.0       0.91      0.69      0.78        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.87      0.89     12630\n",
      "weighted avg       0.91      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.907046714172605\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.62      0.76        60\n",
      "         1.0       0.89      0.80      0.84       720\n",
      "         2.0       0.78      0.96      0.86       750\n",
      "         3.0       0.94      0.75      0.84       450\n",
      "         4.0       0.92      0.93      0.92       660\n",
      "         5.0       0.79      0.83      0.81       630\n",
      "         6.0       0.84      0.73      0.78       150\n",
      "         7.0       0.88      0.90      0.89       450\n",
      "         8.0       0.82      0.85      0.83       450\n",
      "         9.0       0.91      0.98      0.95       480\n",
      "        10.0       0.94      0.98      0.96       660\n",
      "        11.0       0.88      0.91      0.89       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       1.00      0.94      0.97       270\n",
      "        15.0       0.97      0.97      0.97       210\n",
      "        16.0       0.97      0.96      0.97       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.89      0.89      0.89       390\n",
      "        19.0       1.00      0.97      0.98        60\n",
      "        20.0       0.94      0.87      0.90        90\n",
      "        21.0       0.98      0.64      0.78        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.86      0.93      0.90       150\n",
      "        24.0       1.00      0.78      0.88        90\n",
      "        25.0       0.82      0.96      0.88       480\n",
      "        26.0       0.83      0.77      0.80       180\n",
      "        27.0       0.95      0.67      0.78        60\n",
      "        28.0       0.85      0.87      0.86       150\n",
      "        29.0       0.99      0.83      0.90        90\n",
      "        30.0       0.82      0.63      0.71       150\n",
      "        31.0       0.89      0.97      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.96      0.96      0.96       210\n",
      "        34.0       1.00      0.99      1.00       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.97      0.96      0.97       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       0.98      0.98      0.98        90\n",
      "        40.0       0.97      0.78      0.86        90\n",
      "        41.0       1.00      0.57      0.72        60\n",
      "        42.0       0.90      0.70      0.79        90\n",
      "\n",
      "    accuracy                           0.91     12630\n",
      "   macro avg       0.93      0.87      0.89     12630\n",
      "weighted avg       0.91      0.91      0.91     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.8877276326207443\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.20      0.33        60\n",
      "         1.0       0.89      0.78      0.83       720\n",
      "         2.0       0.72      0.93      0.82       750\n",
      "         3.0       0.95      0.71      0.81       450\n",
      "         4.0       0.89      0.92      0.90       660\n",
      "         5.0       0.78      0.79      0.79       630\n",
      "         6.0       0.86      0.76      0.81       150\n",
      "         7.0       0.90      0.88      0.89       450\n",
      "         8.0       0.82      0.86      0.84       450\n",
      "         9.0       0.92      0.94      0.93       480\n",
      "        10.0       0.89      0.98      0.94       660\n",
      "        11.0       0.82      0.91      0.86       420\n",
      "        12.0       0.96      0.99      0.97       690\n",
      "        13.0       0.99      1.00      1.00       720\n",
      "        14.0       0.98      0.93      0.95       270\n",
      "        15.0       0.98      0.98      0.98       210\n",
      "        16.0       1.00      0.94      0.97       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.84      0.90      0.87       390\n",
      "        19.0       1.00      0.90      0.95        60\n",
      "        20.0       0.97      0.80      0.88        90\n",
      "        21.0       0.95      0.59      0.73        90\n",
      "        22.0       1.00      0.72      0.84       120\n",
      "        23.0       0.84      0.85      0.84       150\n",
      "        24.0       1.00      0.59      0.74        90\n",
      "        25.0       0.73      0.99      0.84       480\n",
      "        26.0       0.84      0.63      0.72       180\n",
      "        27.0       1.00      0.50      0.67        60\n",
      "        28.0       0.85      0.84      0.85       150\n",
      "        29.0       1.00      0.74      0.85        90\n",
      "        30.0       0.95      0.41      0.58       150\n",
      "        31.0       0.84      0.96      0.89       270\n",
      "        32.0       1.00      0.98      0.99        60\n",
      "        33.0       0.98      0.98      0.98       210\n",
      "        34.0       1.00      0.97      0.98       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.98      0.97      0.97       120\n",
      "        37.0       1.00      0.68      0.81        60\n",
      "        38.0       0.94      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       0.99      0.76      0.86        90\n",
      "        41.0       0.94      0.50      0.65        60\n",
      "        42.0       0.94      0.68      0.79        90\n",
      "\n",
      "    accuracy                           0.89     12630\n",
      "   macro avg       0.93      0.82      0.86     12630\n",
      "weighted avg       0.90      0.89      0.88     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.879255740300871\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.62      0.75        60\n",
      "         1.0       0.87      0.75      0.81       720\n",
      "         2.0       0.77      0.94      0.84       750\n",
      "         3.0       0.92      0.74      0.82       450\n",
      "         4.0       0.90      0.90      0.90       660\n",
      "         5.0       0.75      0.79      0.77       630\n",
      "         6.0       0.78      0.65      0.71       150\n",
      "         7.0       0.84      0.88      0.86       450\n",
      "         8.0       0.78      0.84      0.81       450\n",
      "         9.0       0.90      0.96      0.93       480\n",
      "        10.0       0.87      0.95      0.91       660\n",
      "        11.0       0.88      0.86      0.87       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.98      0.91      0.94       270\n",
      "        15.0       0.96      0.95      0.95       210\n",
      "        16.0       0.83      0.92      0.87       150\n",
      "        17.0       0.97      0.94      0.96       360\n",
      "        18.0       0.82      0.87      0.85       390\n",
      "        19.0       0.98      0.98      0.98        60\n",
      "        20.0       0.93      0.83      0.88        90\n",
      "        21.0       0.94      0.54      0.69        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.86      0.92      0.89       150\n",
      "        24.0       1.00      0.54      0.71        90\n",
      "        25.0       0.82      0.94      0.88       480\n",
      "        26.0       0.79      0.76      0.77       180\n",
      "        27.0       0.76      0.37      0.49        60\n",
      "        28.0       0.75      0.85      0.79       150\n",
      "        29.0       1.00      0.59      0.74        90\n",
      "        30.0       0.68      0.60      0.64       150\n",
      "        31.0       0.86      0.96      0.90       270\n",
      "        32.0       0.97      1.00      0.98        60\n",
      "        33.0       0.97      0.95      0.96       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.92      0.97      0.95       390\n",
      "        36.0       0.97      0.94      0.96       120\n",
      "        37.0       0.97      0.63      0.77        60\n",
      "        38.0       0.92      0.99      0.95       690\n",
      "        39.0       0.99      0.98      0.98        90\n",
      "        40.0       0.98      0.69      0.81        90\n",
      "        41.0       0.93      0.65      0.76        60\n",
      "        42.0       0.93      0.58      0.71        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.90      0.82      0.85     12630\n",
      "weighted avg       0.88      0.88      0.88     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.8828978622327791\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.58      0.74        60\n",
      "         1.0       0.88      0.75      0.81       720\n",
      "         2.0       0.76      0.95      0.84       750\n",
      "         3.0       0.94      0.76      0.84       450\n",
      "         4.0       0.91      0.90      0.90       660\n",
      "         5.0       0.76      0.79      0.78       630\n",
      "         6.0       0.88      0.65      0.75       150\n",
      "         7.0       0.84      0.88      0.86       450\n",
      "         8.0       0.76      0.84      0.80       450\n",
      "         9.0       0.89      0.98      0.93       480\n",
      "        10.0       0.87      0.95      0.91       660\n",
      "        11.0       0.87      0.88      0.87       420\n",
      "        12.0       0.97      0.99      0.98       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.91      0.95       270\n",
      "        15.0       0.96      0.95      0.96       210\n",
      "        16.0       0.89      0.93      0.91       150\n",
      "        17.0       0.99      0.96      0.97       360\n",
      "        18.0       0.82      0.87      0.84       390\n",
      "        19.0       0.98      0.97      0.97        60\n",
      "        20.0       0.99      0.84      0.91        90\n",
      "        21.0       0.98      0.56      0.71        90\n",
      "        22.0       0.99      0.73      0.84       120\n",
      "        23.0       0.84      0.93      0.88       150\n",
      "        24.0       1.00      0.50      0.67        90\n",
      "        25.0       0.81      0.94      0.87       480\n",
      "        26.0       0.82      0.75      0.78       180\n",
      "        27.0       0.87      0.45      0.59        60\n",
      "        28.0       0.84      0.85      0.84       150\n",
      "        29.0       0.97      0.62      0.76        90\n",
      "        30.0       0.67      0.57      0.62       150\n",
      "        31.0       0.86      0.97      0.91       270\n",
      "        32.0       0.95      1.00      0.98        60\n",
      "        33.0       0.96      0.96      0.96       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.93      0.98      0.95       390\n",
      "        36.0       0.97      0.93      0.95       120\n",
      "        37.0       0.97      0.60      0.74        60\n",
      "        38.0       0.92      0.98      0.95       690\n",
      "        39.0       1.00      0.97      0.98        90\n",
      "        40.0       1.00      0.67      0.80        90\n",
      "        41.0       0.94      0.55      0.69        60\n",
      "        42.0       0.93      0.62      0.75        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.91      0.82      0.85     12630\n",
      "weighted avg       0.89      0.88      0.88     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.8835312747426761\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.30      0.46        60\n",
      "         1.0       0.86      0.75      0.80       720\n",
      "         2.0       0.76      0.94      0.84       750\n",
      "         3.0       0.95      0.74      0.83       450\n",
      "         4.0       0.88      0.90      0.89       660\n",
      "         5.0       0.75      0.79      0.77       630\n",
      "         6.0       0.86      0.64      0.73       150\n",
      "         7.0       0.85      0.89      0.87       450\n",
      "         8.0       0.80      0.84      0.82       450\n",
      "         9.0       0.88      0.98      0.92       480\n",
      "        10.0       0.91      0.96      0.93       660\n",
      "        11.0       0.88      0.88      0.88       420\n",
      "        12.0       0.96      0.99      0.97       690\n",
      "        13.0       0.99      1.00      0.99       720\n",
      "        14.0       0.99      0.93      0.96       270\n",
      "        15.0       0.98      0.98      0.98       210\n",
      "        16.0       0.90      0.90      0.90       150\n",
      "        17.0       0.98      0.95      0.96       360\n",
      "        18.0       0.84      0.88      0.86       390\n",
      "        19.0       1.00      0.93      0.97        60\n",
      "        20.0       0.95      0.87      0.91        90\n",
      "        21.0       0.97      0.64      0.77        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.84      0.89      0.86       150\n",
      "        24.0       1.00      0.53      0.70        90\n",
      "        25.0       0.79      0.97      0.87       480\n",
      "        26.0       0.81      0.75      0.78       180\n",
      "        27.0       0.94      0.48      0.64        60\n",
      "        28.0       0.82      0.83      0.82       150\n",
      "        29.0       0.98      0.52      0.68        90\n",
      "        30.0       0.63      0.57      0.60       150\n",
      "        31.0       0.86      0.96      0.91       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.96      0.97      0.96       210\n",
      "        34.0       0.99      0.97      0.98       120\n",
      "        35.0       0.94      0.98      0.96       390\n",
      "        36.0       0.99      0.95      0.97       120\n",
      "        37.0       0.97      0.53      0.69        60\n",
      "        38.0       0.94      0.99      0.96       690\n",
      "        39.0       1.00      0.98      0.99        90\n",
      "        40.0       1.00      0.69      0.82        90\n",
      "        41.0       0.87      0.55      0.67        60\n",
      "        42.0       0.94      0.67      0.78        90\n",
      "\n",
      "    accuracy                           0.88     12630\n",
      "   macro avg       0.91      0.82      0.85     12630\n",
      "weighted avg       0.89      0.88      0.88     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.8593032462391132\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.05      0.10        60\n",
      "         1.0       0.87      0.74      0.80       720\n",
      "         2.0       0.69      0.96      0.80       750\n",
      "         3.0       0.97      0.67      0.80       450\n",
      "         4.0       0.85      0.92      0.89       660\n",
      "         5.0       0.81      0.77      0.79       630\n",
      "         6.0       0.84      0.65      0.74       150\n",
      "         7.0       0.85      0.90      0.87       450\n",
      "         8.0       0.85      0.78      0.82       450\n",
      "         9.0       0.88      0.93      0.90       480\n",
      "        10.0       0.87      0.98      0.92       660\n",
      "        11.0       0.79      0.89      0.84       420\n",
      "        12.0       0.92      0.99      0.96       690\n",
      "        13.0       0.99      1.00      1.00       720\n",
      "        14.0       0.98      0.92      0.95       270\n",
      "        15.0       0.98      0.96      0.97       210\n",
      "        16.0       1.00      0.87      0.93       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.73      0.93      0.81       390\n",
      "        19.0       1.00      0.73      0.85        60\n",
      "        20.0       1.00      0.72      0.84        90\n",
      "        21.0       0.95      0.47      0.63        90\n",
      "        22.0       0.99      0.72      0.83       120\n",
      "        23.0       0.88      0.78      0.83       150\n",
      "        24.0       1.00      0.13      0.24        90\n",
      "        25.0       0.62      0.99      0.76       480\n",
      "        26.0       0.90      0.51      0.65       180\n",
      "        27.0       1.00      0.20      0.33        60\n",
      "        28.0       0.85      0.70      0.77       150\n",
      "        29.0       1.00      0.18      0.30        90\n",
      "        30.0       0.85      0.31      0.46       150\n",
      "        31.0       0.77      0.95      0.85       270\n",
      "        32.0       1.00      0.98      0.99        60\n",
      "        33.0       0.98      0.96      0.97       210\n",
      "        34.0       1.00      0.96      0.98       120\n",
      "        35.0       0.95      0.98      0.97       390\n",
      "        36.0       1.00      0.94      0.97       120\n",
      "        37.0       1.00      0.55      0.71        60\n",
      "        38.0       0.91      0.99      0.95       690\n",
      "        39.0       1.00      0.97      0.98        90\n",
      "        40.0       1.00      0.62      0.77        90\n",
      "        41.0       1.00      0.37      0.54        60\n",
      "        42.0       0.95      0.61      0.74        90\n",
      "\n",
      "    accuracy                           0.86     12630\n",
      "   macro avg       0.92      0.75      0.78     12630\n",
      "weighted avg       0.88      0.86      0.85     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8114806017418844\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.07      0.12        60\n",
      "         1.0       0.78      0.67      0.72       720\n",
      "         2.0       0.67      0.90      0.77       750\n",
      "         3.0       0.87      0.71      0.78       450\n",
      "         4.0       0.82      0.84      0.83       660\n",
      "         5.0       0.62      0.68      0.65       630\n",
      "         6.0       0.86      0.37      0.51       150\n",
      "         7.0       0.74      0.90      0.81       450\n",
      "         8.0       0.65      0.68      0.67       450\n",
      "         9.0       0.78      0.95      0.85       480\n",
      "        10.0       0.84      0.93      0.88       660\n",
      "        11.0       0.83      0.83      0.83       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.94      0.91      0.92       270\n",
      "        15.0       0.90      0.92      0.91       210\n",
      "        16.0       0.83      0.65      0.73       150\n",
      "        17.0       0.95      0.92      0.93       360\n",
      "        18.0       0.76      0.89      0.82       390\n",
      "        19.0       0.98      0.87      0.92        60\n",
      "        20.0       0.91      0.81      0.86        90\n",
      "        21.0       0.92      0.27      0.41        90\n",
      "        22.0       0.99      0.71      0.83       120\n",
      "        23.0       0.77      0.79      0.78       150\n",
      "        24.0       1.00      0.20      0.33        90\n",
      "        25.0       0.75      0.93      0.83       480\n",
      "        26.0       0.65      0.64      0.65       180\n",
      "        27.0       1.00      0.28      0.44        60\n",
      "        28.0       0.74      0.64      0.69       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.55      0.39      0.45       150\n",
      "        31.0       0.72      0.96      0.82       270\n",
      "        32.0       0.90      0.78      0.84        60\n",
      "        33.0       0.89      0.94      0.91       210\n",
      "        34.0       0.93      0.93      0.93       120\n",
      "        35.0       0.83      0.96      0.89       390\n",
      "        36.0       0.98      0.83      0.90       120\n",
      "        37.0       0.94      0.25      0.39        60\n",
      "        38.0       0.91      0.97      0.94       690\n",
      "        39.0       1.00      0.57      0.72        90\n",
      "        40.0       1.00      0.13      0.24        90\n",
      "        41.0       1.00      0.25      0.40        60\n",
      "        42.0       0.98      0.54      0.70        90\n",
      "\n",
      "    accuracy                           0.81     12630\n",
      "   macro avg       0.84      0.68      0.71     12630\n",
      "weighted avg       0.82      0.81      0.80     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8135391923990499\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.79      0.69      0.74       720\n",
      "         2.0       0.68      0.91      0.78       750\n",
      "         3.0       0.88      0.71      0.79       450\n",
      "         4.0       0.82      0.85      0.83       660\n",
      "         5.0       0.64      0.68      0.66       630\n",
      "         6.0       0.89      0.37      0.53       150\n",
      "         7.0       0.75      0.90      0.82       450\n",
      "         8.0       0.66      0.70      0.68       450\n",
      "         9.0       0.76      0.97      0.85       480\n",
      "        10.0       0.85      0.94      0.89       660\n",
      "        11.0       0.82      0.84      0.83       420\n",
      "        12.0       0.96      0.98      0.97       690\n",
      "        13.0       1.00      0.99      0.99       720\n",
      "        14.0       0.94      0.91      0.92       270\n",
      "        15.0       0.92      0.94      0.93       210\n",
      "        16.0       0.91      0.65      0.75       150\n",
      "        17.0       0.97      0.94      0.95       360\n",
      "        18.0       0.74      0.90      0.81       390\n",
      "        19.0       1.00      0.63      0.78        60\n",
      "        20.0       0.90      0.81      0.85        90\n",
      "        21.0       0.94      0.18      0.30        90\n",
      "        22.0       0.93      0.71      0.81       120\n",
      "        23.0       0.70      0.72      0.71       150\n",
      "        24.0       1.00      0.21      0.35        90\n",
      "        25.0       0.74      0.93      0.83       480\n",
      "        26.0       0.74      0.68      0.71       180\n",
      "        27.0       1.00      0.18      0.31        60\n",
      "        28.0       0.77      0.61      0.68       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.48      0.38      0.43       150\n",
      "        31.0       0.68      0.97      0.80       270\n",
      "        32.0       0.98      0.92      0.95        60\n",
      "        33.0       0.91      0.92      0.91       210\n",
      "        34.0       0.92      0.91      0.92       120\n",
      "        35.0       0.87      0.97      0.92       390\n",
      "        36.0       0.98      0.80      0.88       120\n",
      "        37.0       0.94      0.25      0.39        60\n",
      "        38.0       0.90      0.96      0.93       690\n",
      "        39.0       1.00      0.54      0.71        90\n",
      "        40.0       1.00      0.07      0.12        90\n",
      "        41.0       1.00      0.10      0.18        60\n",
      "        42.0       0.96      0.51      0.67        90\n",
      "\n",
      "    accuracy                           0.81     12630\n",
      "   macro avg       0.82      0.67      0.69     12630\n",
      "weighted avg       0.82      0.81      0.80     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8175771971496437\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.83      0.70      0.76       720\n",
      "         2.0       0.69      0.91      0.78       750\n",
      "         3.0       0.89      0.72      0.80       450\n",
      "         4.0       0.82      0.88      0.85       660\n",
      "         5.0       0.65      0.67      0.66       630\n",
      "         6.0       0.92      0.45      0.60       150\n",
      "         7.0       0.77      0.90      0.83       450\n",
      "         8.0       0.73      0.72      0.73       450\n",
      "         9.0       0.75      0.98      0.85       480\n",
      "        10.0       0.88      0.95      0.91       660\n",
      "        11.0       0.77      0.87      0.82       420\n",
      "        12.0       0.94      0.99      0.96       690\n",
      "        13.0       0.99      0.99      0.99       720\n",
      "        14.0       0.96      0.89      0.92       270\n",
      "        15.0       0.91      0.94      0.93       210\n",
      "        16.0       0.94      0.62      0.75       150\n",
      "        17.0       0.96      0.95      0.96       360\n",
      "        18.0       0.70      0.92      0.79       390\n",
      "        19.0       1.00      0.50      0.67        60\n",
      "        20.0       0.97      0.73      0.84        90\n",
      "        21.0       0.94      0.17      0.28        90\n",
      "        22.0       0.97      0.72      0.83       120\n",
      "        23.0       0.71      0.75      0.73       150\n",
      "        24.0       1.00      0.04      0.09        90\n",
      "        25.0       0.68      0.96      0.80       480\n",
      "        26.0       0.82      0.52      0.63       180\n",
      "        27.0       1.00      0.17      0.29        60\n",
      "        28.0       0.84      0.57      0.68       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.56      0.32      0.41       150\n",
      "        31.0       0.69      0.95      0.80       270\n",
      "        32.0       0.95      0.67      0.78        60\n",
      "        33.0       0.88      0.93      0.91       210\n",
      "        34.0       0.97      0.91      0.94       120\n",
      "        35.0       0.85      0.98      0.91       390\n",
      "        36.0       0.99      0.89      0.94       120\n",
      "        37.0       1.00      0.28      0.44        60\n",
      "        38.0       0.90      0.98      0.94       690\n",
      "        39.0       0.99      0.76      0.86        90\n",
      "        40.0       1.00      0.27      0.42        90\n",
      "        41.0       0.50      0.02      0.03        60\n",
      "        42.0       0.98      0.46      0.62        90\n",
      "\n",
      "    accuracy                           0.82     12630\n",
      "   macro avg       0.82      0.66      0.69     12630\n",
      "weighted avg       0.82      0.82      0.80     12630\n",
      "\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7703087885985748\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        60\n",
      "         1.0       0.79      0.68      0.73       720\n",
      "         2.0       0.57      0.94      0.71       750\n",
      "         3.0       0.98      0.58      0.73       450\n",
      "         4.0       0.75      0.91      0.82       660\n",
      "         5.0       0.79      0.59      0.67       630\n",
      "         6.0       0.88      0.44      0.59       150\n",
      "         7.0       0.77      0.86      0.81       450\n",
      "         8.0       0.82      0.63      0.72       450\n",
      "         9.0       0.80      0.84      0.82       480\n",
      "        10.0       0.71      0.96      0.82       660\n",
      "        11.0       0.68      0.86      0.76       420\n",
      "        12.0       0.89      1.00      0.94       690\n",
      "        13.0       0.98      1.00      0.99       720\n",
      "        14.0       0.98      0.89      0.93       270\n",
      "        15.0       0.98      0.86      0.92       210\n",
      "        16.0       1.00      0.39      0.56       150\n",
      "        17.0       0.98      0.99      0.98       360\n",
      "        18.0       0.62      0.91      0.74       390\n",
      "        19.0       1.00      0.12      0.21        60\n",
      "        20.0       1.00      0.50      0.67        90\n",
      "        21.0       0.83      0.06      0.10        90\n",
      "        22.0       1.00      0.63      0.78       120\n",
      "        23.0       0.88      0.44      0.59       150\n",
      "        24.0       0.00      0.00      0.00        90\n",
      "        25.0       0.48      0.99      0.64       480\n",
      "        26.0       0.91      0.06      0.10       180\n",
      "        27.0       0.00      0.00      0.00        60\n",
      "        28.0       1.00      0.18      0.31       150\n",
      "        29.0       0.00      0.00      0.00        90\n",
      "        30.0       0.88      0.09      0.17       150\n",
      "        31.0       0.68      0.96      0.79       270\n",
      "        32.0       1.00      0.58      0.74        60\n",
      "        33.0       0.97      0.90      0.93       210\n",
      "        34.0       1.00      0.68      0.81       120\n",
      "        35.0       0.85      0.97      0.91       390\n",
      "        36.0       1.00      0.77      0.87       120\n",
      "        37.0       1.00      0.17      0.29        60\n",
      "        38.0       0.83      0.99      0.90       690\n",
      "        39.0       1.00      0.73      0.85        90\n",
      "        40.0       1.00      0.11      0.20        90\n",
      "        41.0       0.00      0.00      0.00        60\n",
      "        42.0       0.97      0.39      0.56        90\n",
      "\n",
      "    accuracy                           0.77     12630\n",
      "   macro avg       0.77      0.57      0.60     12630\n",
      "weighted avg       0.79      0.77      0.74     12630\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ken/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for num_estimators in [1000, 500, 100]:\n",
    "    for sample_split in [10, 100, 500]:\n",
    "        for num_features in [100, 75, 50, 10]:\n",
    "            print(\"n_estimators: \" + str(num_estimators) + \" min_samples_split: \" + str(sample_split) + \" max_features: \" + str(num_features))\n",
    "            clf_rfc_paper = ensemble.RandomForestClassifier(n_estimators=num_estimators, min_samples_split=sample_split, max_features=num_features, n_jobs=8)\n",
    "            clf_rfc_paper.fit(trainData, trainLabels)\n",
    "            with open('rfc_model_paper_'+ str(num_estimators) + \"_\" + str(sample_split) + \"_\" + str(num_features) + '.pickle', 'wb') as handle:\n",
    "                pickle.dump(clf_rfc_paper, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            predicted_labels_rfc_paper = clf_rfc_paper.predict(testData)\n",
    "            print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_paper)))\n",
    "            print('\\n')\n",
    "            print(classification_report(testLabels, predicted_labels_rfc_paper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffled train data\n",
    "clf_rfc_shuffled = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=8)\n",
    "clf_rfc_shuffled.fit(trainDataShuffled, trainLabelsShuffled)\n",
    "with open('rfc_model_shuffled.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc_shuffled, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "predicted_labels_rfc_shuffled = clf_rfc_shuffled.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_shuffled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39209, 1567)\n",
      "(39209,)\n",
      "(12630, 1567)\n",
      "(12630,)\n"
     ]
    }
   ],
   "source": [
    "# Official Hog Data\n",
    "# Training\n",
    "trainDataOfficialHog = np.array(hogfeat_official).astype(\"float\")\n",
    "trainLabelsOfficialHog = np.array(hoglabels_official).astype(\"float\")\n",
    "print(trainDataOfficialHog.shape)\n",
    "print(trainLabelsOfficialHog.shape)\n",
    "\n",
    "# Testing\n",
    "testDataOfficialHog = np.array(hogfeat_official_test).astype(\"float\")\n",
    "testLabelsOfficialHog = np.array(hoglabels_official_test).astype(\"float\")\n",
    "print(testDataOfficialHog.shape)\n",
    "print(testLabelsOfficialHog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9676167854315123\n"
     ]
    }
   ],
   "source": [
    "clf_rfc_official_hog = ensemble.RandomForestClassifier(n_estimators=1000, n_jobs=8)\n",
    "clf_rfc_official_hog.fit(trainDataOfficialHog, trainLabelsOfficialHog)\n",
    "with open('rfc_model_official_hog.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc_official_hog, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "predicted_labels_rfc_shuffled = clf_rfc_official_hog.predict(testDataOfficialHog)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabelsOfficialHog, predicted_labels_rfc_shuffled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 1000 min_samples_split: 10 max_features: 100\n",
      "Accuracy: 0.9669833729216152\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.9678543151227237\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.9668250197941409\n",
      "n_estimators: 1000 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.9568487727632621\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 100\n",
      "Accuracy: 0.9596199524940617\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.959778305621536\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.9592240696753761\n",
      "n_estimators: 1000 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.9410926365795724\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 100\n",
      "Accuracy: 0.9315914489311163\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 75\n",
      "Accuracy: 0.9333333333333333\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 50\n",
      "Accuracy: 0.9315122723673792\n",
      "n_estimators: 1000 min_samples_split: 500 max_features: 10\n",
      "Accuracy: 0.897070467141726\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 100\n",
      "Accuracy: 0.9681710213776722\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.9679334916864608\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.9667458432304038\n",
      "n_estimators: 500 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.9555819477434679\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 100\n",
      "Accuracy: 0.9599366587490102\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.9593032462391132\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.9596199524940617\n",
      "n_estimators: 500 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.9426761678543151\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 100\n",
      "Accuracy: 0.9334916864608076\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 75\n",
      "Accuracy: 0.9334916864608076\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 50\n",
      "Accuracy: 0.9300870942201108\n",
      "n_estimators: 500 min_samples_split: 500 max_features: 10\n",
      "Accuracy: 0.8929532858273951\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 100\n",
      "Accuracy: 0.9625494853523358\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 75\n",
      "Accuracy: 0.9653206650831354\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 50\n",
      "Accuracy: 0.9613618368962787\n",
      "n_estimators: 100 min_samples_split: 10 max_features: 10\n",
      "Accuracy: 0.9479809976247031\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 100\n",
      "Accuracy: 0.9529691211401425\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 75\n",
      "Accuracy: 0.9543151227236738\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 50\n",
      "Accuracy: 0.9573238321456848\n",
      "n_estimators: 100 min_samples_split: 100 max_features: 10\n",
      "Accuracy: 0.9406967537608868\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 100\n",
      "Accuracy: 0.9265241488519398\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 75\n",
      "Accuracy: 0.9257323832145685\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 50\n",
      "Accuracy: 0.9247822644497229\n",
      "n_estimators: 100 min_samples_split: 500 max_features: 10\n",
      "Accuracy: 0.8924782264449723\n"
     ]
    }
   ],
   "source": [
    "for num_estimators in [1000, 500, 100]:\n",
    "    for sample_split in [10, 100, 500]:\n",
    "        for num_features in [100, 75, 50, 10]:\n",
    "            print(\"n_estimators: \" + str(num_estimators) + \" min_samples_split: \" + str(sample_split) + \" max_features: \" + str(num_features))\n",
    "            clf_rfc_official_hog_hyper = ensemble.RandomForestClassifier(n_estimators=num_estimators, min_samples_split=sample_split, max_features=num_features, n_jobs=8)\n",
    "            clf_rfc_official_hog_hyper.fit(trainDataOfficialHog, trainLabelsOfficialHog)\n",
    "            with open('rfc_model_official_hog_hyper_'+ str(num_estimators) + \"_\" + str(sample_split) + \"_\" + str(num_features) + '.pickle', 'wb') as handle:\n",
    "                pickle.dump(clf_rfc_official_hog_hyper, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            predicted_labels_rfc_official_hog_hyper = clf_rfc_official_hog_hyper.predict(testDataOfficialHog)\n",
    "            print(\"Accuracy: \" + str(accuracy_score(testLabelsOfficialHog, predicted_labels_rfc_official_hog_hyper)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 500 min_samples_split: 10 max_features: 100\n"
     ]
    }
   ],
   "source": [
    "# Best results\n",
    "num_estimators = 500\n",
    "sample_split = 10\n",
    "num_features = 100\n",
    "print(\"n_estimators: \" + str(num_estimators) + \" min_samples_split: \" + str(sample_split) + \" max_features: \" + str(num_features))\n",
    "# notice we should only use on cpu\n",
    "clf_rfc_best = ensemble.RandomForestClassifier(n_estimators=num_estimators, min_samples_split=sample_split, max_features=num_features, n_jobs=1)\n",
    "clf_rfc_best.fit(trainDataOfficialHog, trainLabelsOfficialHog)\n",
    "with open('rfc_model_best_'+ str(num_estimators) + \"_\" + str(sample_split) + \"_\" + str(num_features) + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_rfc_best, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_labels_rfc_best = clf_rfc_best.predict(testDataOfficialHog)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Inference Time: \" + str(elapsed_time))\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabelsOfficialHog, predicted_labels_rfc_best)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabelsOfficialHog, predicted_labels_rfc_best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time: 0.025321245193481445\n",
      "Accuracy: 0.9578780680918448\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.93      0.90        60\n",
      "         1.0       0.97      0.96      0.97       720\n",
      "         2.0       0.94      0.99      0.96       750\n",
      "         3.0       0.94      0.94      0.94       450\n",
      "         4.0       0.97      0.97      0.97       660\n",
      "         5.0       0.93      0.92      0.92       630\n",
      "         6.0       0.97      0.83      0.90       150\n",
      "         7.0       0.94      0.92      0.93       450\n",
      "         8.0       0.92      0.96      0.94       450\n",
      "         9.0       0.96      0.99      0.98       480\n",
      "        10.0       0.99      0.98      0.99       660\n",
      "        11.0       0.96      0.94      0.95       420\n",
      "        12.0       1.00      0.99      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.98      0.98       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       1.00      0.99      1.00       150\n",
      "        17.0       1.00      0.99      1.00       360\n",
      "        18.0       0.99      0.91      0.95       390\n",
      "        19.0       0.94      1.00      0.97        60\n",
      "        20.0       0.78      0.97      0.87        90\n",
      "        21.0       0.89      0.89      0.89        90\n",
      "        22.0       0.96      0.93      0.95       120\n",
      "        23.0       0.96      0.85      0.90       150\n",
      "        24.0       0.92      0.96      0.94        90\n",
      "        25.0       0.95      0.97      0.96       480\n",
      "        26.0       0.83      0.82      0.82       180\n",
      "        27.0       0.93      0.63      0.75        60\n",
      "        28.0       0.90      0.98      0.94       150\n",
      "        29.0       0.91      0.92      0.92        90\n",
      "        30.0       0.86      0.80      0.83       150\n",
      "        31.0       0.92      0.98      0.95       270\n",
      "        32.0       0.91      1.00      0.95        60\n",
      "        33.0       0.97      1.00      0.98       210\n",
      "        34.0       0.98      0.99      0.98       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       0.98      1.00      0.99       120\n",
      "        37.0       0.97      0.93      0.95        60\n",
      "        38.0       0.98      0.99      0.99       690\n",
      "        39.0       0.97      1.00      0.98        90\n",
      "        40.0       0.94      0.84      0.89        90\n",
      "        41.0       1.00      0.75      0.86        60\n",
      "        42.0       0.99      0.90      0.94        90\n",
      "\n",
      "    accuracy                           0.96     12630\n",
      "   macro avg       0.95      0.94      0.94     12630\n",
      "weighted avg       0.96      0.96      0.96     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svm_linear_official = svm.LinearSVC(max_iter=10000)\n",
    "clf_svm_linear_official.fit(trainDataOfficialHog, trainLabelsOfficialHog)\n",
    "with open('svm_linear_model_official.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear_official, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "start_time = time.time()\n",
    "predicted_labels_svm_official = clf_svm_linear_official.predict(testDataOfficialHog)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Inference Time: \" + str(elapsed_time))\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabelsOfficialHog, predicted_labels_svm_official)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabelsOfficialHog, predicted_labels_svm_official))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Intensity Rescaling for HOG Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "trainData = np.array(hogfeat_no_intense).astype(\"float\")\n",
    "trainLabels = y.astype(\"float\")\n",
    "\n",
    "# Testing\n",
    "testData = np.array(hogfeat_test_no_intense).astype(\"float\")\n",
    "testLabels = np.array(tlbl).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9169437846397467\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.48      0.65        60\n",
      "         1.0       0.91      0.81      0.85       720\n",
      "         2.0       0.78      0.97      0.86       750\n",
      "         3.0       0.96      0.78      0.86       450\n",
      "         4.0       0.91      0.93      0.92       660\n",
      "         5.0       0.83      0.86      0.84       630\n",
      "         6.0       0.84      0.75      0.79       150\n",
      "         7.0       0.90      0.91      0.91       450\n",
      "         8.0       0.86      0.87      0.87       450\n",
      "         9.0       0.92      0.98      0.95       480\n",
      "        10.0       0.94      0.98      0.96       660\n",
      "        11.0       0.88      0.91      0.89       420\n",
      "        12.0       0.98      0.99      0.99       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.96      0.97       270\n",
      "        15.0       0.98      0.98      0.98       210\n",
      "        16.0       0.98      0.97      0.97       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.91      0.90      0.91       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.97      0.87      0.92        90\n",
      "        21.0       1.00      0.69      0.82        90\n",
      "        22.0       0.98      0.74      0.84       120\n",
      "        23.0       0.87      0.93      0.90       150\n",
      "        24.0       1.00      0.80      0.89        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.87      0.77      0.81       180\n",
      "        27.0       0.98      0.72      0.83        60\n",
      "        28.0       0.83      0.91      0.87       150\n",
      "        29.0       1.00      0.92      0.96        90\n",
      "        30.0       0.92      0.63      0.75       150\n",
      "        31.0       0.89      0.98      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.99      0.98       210\n",
      "        34.0       0.99      0.98      0.99       120\n",
      "        35.0       0.96      0.98      0.97       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.82      0.90        60\n",
      "        38.0       0.96      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.80      0.89        90\n",
      "        41.0       1.00      0.55      0.71        60\n",
      "        42.0       0.92      0.68      0.78        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.88      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n",
      "Accuracy: 0.9193190815518606\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.53      0.70        60\n",
      "         1.0       0.91      0.81      0.86       720\n",
      "         2.0       0.80      0.97      0.87       750\n",
      "         3.0       0.96      0.78      0.86       450\n",
      "         4.0       0.91      0.94      0.92       660\n",
      "         5.0       0.82      0.86      0.84       630\n",
      "         6.0       0.85      0.76      0.80       150\n",
      "         7.0       0.91      0.93      0.92       450\n",
      "         8.0       0.88      0.88      0.88       450\n",
      "         9.0       0.90      0.98      0.94       480\n",
      "        10.0       0.95      0.99      0.97       660\n",
      "        11.0       0.88      0.92      0.90       420\n",
      "        12.0       0.98      0.99      0.98       690\n",
      "        13.0       1.00      1.00      1.00       720\n",
      "        14.0       0.99      0.95      0.97       270\n",
      "        15.0       0.98      0.99      0.99       210\n",
      "        16.0       0.99      0.97      0.98       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.91      0.90      0.91       390\n",
      "        19.0       1.00      0.98      0.99        60\n",
      "        20.0       0.96      0.88      0.92        90\n",
      "        21.0       1.00      0.63      0.78        90\n",
      "        22.0       0.99      0.74      0.85       120\n",
      "        23.0       0.89      0.93      0.91       150\n",
      "        24.0       1.00      0.81      0.90        90\n",
      "        25.0       0.83      0.99      0.90       480\n",
      "        26.0       0.87      0.77      0.82       180\n",
      "        27.0       0.96      0.73      0.83        60\n",
      "        28.0       0.85      0.90      0.88       150\n",
      "        29.0       0.99      0.91      0.95        90\n",
      "        30.0       0.96      0.63      0.76       150\n",
      "        31.0       0.88      0.99      0.93       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      0.98      0.97       210\n",
      "        34.0       1.00      0.98      0.99       120\n",
      "        35.0       0.97      0.98      0.98       390\n",
      "        36.0       0.99      0.97      0.98       120\n",
      "        37.0       1.00      0.78      0.88        60\n",
      "        38.0       0.95      0.99      0.97       690\n",
      "        39.0       1.00      0.99      0.99        90\n",
      "        40.0       1.00      0.80      0.89        90\n",
      "        41.0       1.00      0.55      0.71        60\n",
      "        42.0       0.93      0.72      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.94      0.88      0.90     12630\n",
      "weighted avg       0.92      0.92      0.92     12630\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-aae46bf8d581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mclf_rfc_no_intense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mclf_rfc_no_intense\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rfc_model_no_intense_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_rfc_no_intense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 330\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in [500, 1000, 2000]:\n",
    "    clf_rfc_no_intense = ensemble.RandomForestClassifier(n_estimators=i, n_jobs=8)\n",
    "    clf_rfc_no_intense.fit(trainData, trainLabels)\n",
    "    with open('rfc_model_no_intense_'+ str(i) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_rfc_no_intense, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    predicted_labels_rfc_no_intense = clf_rfc_no_intense.predict(testData)\n",
    "    print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_no_intense)))\n",
    "    print('\\n')\n",
    "    print(classification_report(testLabels, predicted_labels_rfc_no_intense))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented data experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hogfeat_full = np.concatenate([hogfeat, hogfeat_aug])\n",
    "y_full = np.concatenate([y, trainAugLabels])\n",
    "\n",
    "trainData = np.array(hogfeat_full).astype(\"float\")\n",
    "trainLabels = y_full.astype(\"float\")\n",
    "testData = np.array(hogfeat_test).astype(\"float\")\n",
    "testLabels = np.array(tlbl).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9231195566112431\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.98      0.92        60\n",
      "         1.0       0.95      0.76      0.85       720\n",
      "         2.0       0.85      0.94      0.90       750\n",
      "         3.0       0.93      0.82      0.88       450\n",
      "         4.0       0.97      0.91      0.94       660\n",
      "         5.0       0.81      0.85      0.83       630\n",
      "         6.0       0.73      0.86      0.79       150\n",
      "         7.0       0.88      0.94      0.91       450\n",
      "         8.0       0.82      0.90      0.86       450\n",
      "         9.0       0.97      0.99      0.98       480\n",
      "        10.0       0.97      0.97      0.97       660\n",
      "        11.0       0.97      0.84      0.90       420\n",
      "        12.0       1.00      0.98      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.96      0.97      0.96       270\n",
      "        15.0       0.95      1.00      0.97       210\n",
      "        16.0       0.90      0.99      0.94       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.96      0.86      0.91       390\n",
      "        19.0       0.92      1.00      0.96        60\n",
      "        20.0       0.93      0.92      0.93        90\n",
      "        21.0       0.87      0.80      0.83        90\n",
      "        22.0       0.94      0.74      0.83       120\n",
      "        23.0       0.83      1.00      0.91       150\n",
      "        24.0       0.97      0.93      0.95        90\n",
      "        25.0       0.92      0.96      0.94       480\n",
      "        26.0       0.84      0.81      0.82       180\n",
      "        27.0       0.69      0.72      0.70        60\n",
      "        28.0       0.75      0.96      0.84       150\n",
      "        29.0       0.85      1.00      0.92        90\n",
      "        30.0       0.74      0.80      0.77       150\n",
      "        31.0       0.91      0.97      0.94       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.98      1.00      0.99       210\n",
      "        34.0       0.94      1.00      0.97       120\n",
      "        35.0       0.98      0.97      0.98       390\n",
      "        36.0       0.94      0.97      0.96       120\n",
      "        37.0       0.95      0.95      0.95        60\n",
      "        38.0       0.99      0.99      0.99       690\n",
      "        39.0       0.98      0.99      0.98        90\n",
      "        40.0       0.80      0.86      0.83        90\n",
      "        41.0       1.00      0.87      0.93        60\n",
      "        42.0       0.79      0.80      0.80        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.92      0.91     12630\n",
      "weighted avg       0.93      0.92      0.92     12630\n",
      "\n",
      "Accuracy: 0.923673792557403\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.98      0.94        60\n",
      "         1.0       0.95      0.76      0.85       720\n",
      "         2.0       0.86      0.95      0.90       750\n",
      "         3.0       0.92      0.82      0.87       450\n",
      "         4.0       0.98      0.92      0.95       660\n",
      "         5.0       0.82      0.85      0.83       630\n",
      "         6.0       0.70      0.87      0.78       150\n",
      "         7.0       0.87      0.93      0.90       450\n",
      "         8.0       0.83      0.90      0.86       450\n",
      "         9.0       0.98      0.98      0.98       480\n",
      "        10.0       0.96      0.97      0.97       660\n",
      "        11.0       0.96      0.84      0.90       420\n",
      "        12.0       1.00      0.98      0.99       690\n",
      "        13.0       1.00      0.99      1.00       720\n",
      "        14.0       0.98      0.98      0.98       270\n",
      "        15.0       0.97      1.00      0.98       210\n",
      "        16.0       0.89      0.99      0.94       150\n",
      "        17.0       0.99      0.97      0.98       360\n",
      "        18.0       0.96      0.87      0.91       390\n",
      "        19.0       0.95      1.00      0.98        60\n",
      "        20.0       0.91      0.92      0.92        90\n",
      "        21.0       0.90      0.80      0.85        90\n",
      "        22.0       0.96      0.74      0.84       120\n",
      "        23.0       0.81      0.99      0.89       150\n",
      "        24.0       0.99      0.93      0.96        90\n",
      "        25.0       0.91      0.96      0.93       480\n",
      "        26.0       0.83      0.82      0.82       180\n",
      "        27.0       0.66      0.67      0.66        60\n",
      "        28.0       0.78      0.97      0.86       150\n",
      "        29.0       0.81      0.98      0.89        90\n",
      "        30.0       0.75      0.79      0.77       150\n",
      "        31.0       0.93      0.97      0.95       270\n",
      "        32.0       0.98      1.00      0.99        60\n",
      "        33.0       0.97      1.00      0.99       210\n",
      "        34.0       0.93      1.00      0.96       120\n",
      "        35.0       0.99      0.98      0.99       390\n",
      "        36.0       0.94      0.97      0.96       120\n",
      "        37.0       0.94      0.97      0.95        60\n",
      "        38.0       0.99      0.99      0.99       690\n",
      "        39.0       0.99      0.99      0.99        90\n",
      "        40.0       0.81      0.87      0.84        90\n",
      "        41.0       0.98      0.88      0.93        60\n",
      "        42.0       0.83      0.80      0.81        90\n",
      "\n",
      "    accuracy                           0.92     12630\n",
      "   macro avg       0.91      0.92      0.91     12630\n",
      "weighted avg       0.93      0.92      0.92     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [500, 1000]:\n",
    "    clf_rfc_aug = ensemble.RandomForestClassifier(n_estimators=i, n_jobs=8)\n",
    "    clf_rfc_aug.fit(trainData, trainLabels)\n",
    "    with open('rfc_model_aug_'+ str(i) + '.pickle', 'wb') as handle:\n",
    "        pickle.dump(clf_rfc_aug, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    predicted_labels_rfc_aug = clf_rfc_aug.predict(testData)\n",
    "    print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_rfc_aug)))\n",
    "    print('\\n')\n",
    "    print(classification_report(testLabels, predicted_labels_rfc_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9008709422011085\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.73      0.68        60\n",
      "         1.0       0.87      0.80      0.83       720\n",
      "         2.0       0.80      0.85      0.82       750\n",
      "         3.0       0.80      0.79      0.80       450\n",
      "         4.0       0.94      0.92      0.93       660\n",
      "         5.0       0.76      0.76      0.76       630\n",
      "         6.0       0.89      0.79      0.84       150\n",
      "         7.0       0.88      0.90      0.89       450\n",
      "         8.0       0.85      0.84      0.85       450\n",
      "         9.0       0.97      0.97      0.97       480\n",
      "        10.0       0.96      0.95      0.96       660\n",
      "        11.0       0.94      0.85      0.89       420\n",
      "        12.0       1.00      0.98      0.99       690\n",
      "        13.0       0.99      1.00      1.00       720\n",
      "        14.0       0.97      0.98      0.97       270\n",
      "        15.0       0.98      0.99      0.98       210\n",
      "        16.0       0.95      0.97      0.96       150\n",
      "        17.0       0.99      0.99      0.99       360\n",
      "        18.0       0.95      0.87      0.91       390\n",
      "        19.0       0.86      1.00      0.92        60\n",
      "        20.0       0.76      0.90      0.83        90\n",
      "        21.0       0.89      0.70      0.78        90\n",
      "        22.0       0.96      0.78      0.86       120\n",
      "        23.0       0.78      0.91      0.84       150\n",
      "        24.0       0.80      0.89      0.84        90\n",
      "        25.0       0.90      0.93      0.92       480\n",
      "        26.0       0.80      0.78      0.79       180\n",
      "        27.0       0.68      0.65      0.67        60\n",
      "        28.0       0.76      0.89      0.82       150\n",
      "        29.0       0.70      0.90      0.79        90\n",
      "        30.0       0.78      0.73      0.76       150\n",
      "        31.0       0.93      0.96      0.94       270\n",
      "        32.0       0.91      1.00      0.95        60\n",
      "        33.0       0.96      0.96      0.96       210\n",
      "        34.0       0.91      0.97      0.94       120\n",
      "        35.0       0.97      0.95      0.96       390\n",
      "        36.0       0.96      0.99      0.98       120\n",
      "        37.0       0.95      0.93      0.94        60\n",
      "        38.0       0.97      0.97      0.97       690\n",
      "        39.0       0.94      1.00      0.97        90\n",
      "        40.0       0.81      0.92      0.86        90\n",
      "        41.0       0.95      0.98      0.97        60\n",
      "        42.0       0.81      0.77      0.79        90\n",
      "\n",
      "    accuracy                           0.90     12630\n",
      "   macro avg       0.88      0.89      0.88     12630\n",
      "weighted avg       0.90      0.90      0.90     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_svm_linear_aug = svm.LinearSVC(max_iter=10000)\n",
    "clf_svm_linear_aug.fit(trainData, trainLabels)\n",
    "with open('svm_linear_model_aug.pickle', 'wb') as handle:\n",
    "    pickle.dump(clf_svm_linear_aug, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "predicted_labels_svm_aug = clf_svm_linear_aug.predict(testData)\n",
    "print(\"Accuracy: \" + str(accuracy_score(testLabels, predicted_labels_svm_aug)))\n",
    "print('\\n')\n",
    "print(classification_report(testLabels, predicted_labels_svm_aug))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
